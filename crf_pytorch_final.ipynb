{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "healthy-broadway",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook is for the reproduction of paper: \"A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection\" (https://arxiv.org/abs/2005.07431) using PyTorch. The original implementation of this project in Keras is avaiable  at https://github.com/TUMFTM/CameraRadarFusionNet.\n",
    "A blog about this reproduction can be found at https://cutt.ly/QvaAsuu.\n",
    "\n",
    "## Requirements\n",
    "1. Pytorch\n",
    "2. Nuscenes dev-kit (https://github.com/nutonomy/nuscenes-devkit)\n",
    "3. Nuscenes dataset (https://www.nuscenes.org/nuscenes)\n",
    "4. CRFNet (https://github.com/TUMFTM/CameraRadarFusionNet)(We mainly used some exsting utils for radar signal processing in the existing project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interior-process",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as tv_models\n",
    "import torchvision as tv\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Nuscenes dev-kit\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.geometry_utils import box_in_image, view_points, BoxVisibility, points_in_box\n",
    "\n",
    "#Libraries for data fusion\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from pyquaternion import Quaternion\n",
    "from PIL import Image\n",
    "from nuscenes.utils.data_classes import PointCloud\n",
    "from crfnet.utils import radar\n",
    "from crfnet.utils.nuscenes_helper import get_sensor_sample_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dental-boost",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.6 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "# Data importing\n",
    "#nusc = NuScenes(version='v1.0-mini', dataroot='F:\\\\CameraRadarFusionNet\\\\crfnet\\\\data\\\\nuscenes', verbose=True)\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='E:\\\\Documents\\\\Robotics\\\\Deep_Learning\\\\Anaconda_CRF\\\\CameraRadarFusionNet\\\\crfnet\\\\data\\\\nuscenes', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-cooking",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The construction of custome NuScenes includes 4 steps\n",
    "1. Initialization of new class map and sample tokens\n",
    "2. Acquire camera, radar and annotation data from each sample(by nuscenes-devkit)\n",
    "3. Radar and camera data fusion\n",
    "4. Transform 3D annotations in NuScenes to 2D annotation box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "willing-pavilion",
   "metadata": {
    "code_folding": [
     0,
     2,
     35,
     38,
     73,
     124,
     241,
     257,
     304,
     373,
     417,
     454,
     495,
     555,
     595,
     699,
     751
    ]
   },
   "outputs": [],
   "source": [
    "# Custome pytorch NuScenes dataset\n",
    "class nuscenes_dataset(Dataset):       \n",
    "    def __init__(self,nusc,\n",
    "                category_mapping=None\n",
    "                ,image_min_side= 360\n",
    "                ,image_max_side = 640):\n",
    "            self.nusc=nusc\n",
    "            self.sample_tokens = {}\n",
    "            self.image_data = dict()\n",
    "            self.radar_sensors = ['RADAR_FRONT']\n",
    "            self.camera_sensors = ['CAM_FRONT']\n",
    "            self.only_radar_annotated = False\n",
    "            self.normalize_bbox = False # True for normalizing the bbox to [0,1]\n",
    "            self.image_min_side = image_min_side\n",
    "            self.image_max_side = image_max_side\n",
    "            self.normalize_bbox = False\n",
    "            self.n_sweeps = 1\n",
    "            self.classes, self.labels = self._get_class_label_mapping([c['name'] for c in nusc.category], category_mapping)\n",
    "            prog = 0\n",
    "            scene_indices = range(len(nusc.scene))\n",
    "            self.class_map = {0: -1,1: 7,2: 7, 3: 7,4: 7,5: 7,6: 7,7: 7,8: -1,9: -1,10: -1,11: -1,12: -1,\n",
    "                              13: 2,14: 3,15: 3,16: 0,17: -1,18: 5,19: 5,20: 1,21: 6,22: 4,23: -1}\n",
    "            for scene_index in scene_indices:\n",
    "                first_sample_token = nusc.scene[scene_index]['first_sample_token']\n",
    "                nbr_samples = nusc.scene[scene_index]['nbr_samples']\n",
    "\n",
    "                curr_sample = nusc.get('sample', first_sample_token)\n",
    "\n",
    "                for _ in range(nbr_samples):\n",
    "                    self.sample_tokens[prog] = curr_sample['token']\n",
    "                    if curr_sample['next']:\n",
    "                        next_token = curr_sample['next']\n",
    "                        curr_sample = nusc.get('sample', next_token)\n",
    "                    prog += 1    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample_tokens)\n",
    "           \n",
    "    def __getitem__(self, index):\n",
    "        # get data \n",
    "        sample_token = self.sample_tokens[index]\n",
    "        sample = nusc.get('sample',sample_token)\n",
    "        image_data = get_sensor_sample_data(nusc, sample, self.camera_sensors[0])\n",
    "        radar_data = get_sensor_sample_data(nusc, sample, self.radar_sensors[0])\n",
    "        camera_token = sample['data'][self.camera_sensors[0]]\n",
    "        radar_token = sample['data'][self.radar_sensors[0]]\n",
    "        #nusc.render_sample_data(camera_token)\n",
    "        \n",
    "        # image fusing \n",
    "        height = (0,3)\n",
    "        image_target_shape = (self.image_min_side, self.image_max_side)\n",
    "        image_plus_data = imageplus_creation(self.nusc, image_data, radar_data, radar_token, camera_token, height, image_target_shape)\n",
    "        image_plus_data = np.transpose(image_plus_data)\n",
    "        image_plus_data = torch.tensor(image_plus_data)\n",
    "        image_plus_data = torch.transpose(image_plus_data,2,1)\n",
    "        image_plus_data = image_plus_data[(0,1,2,5,18) ,:,: ]\n",
    "        image_plus_data[(0,1,2),:,:] = (image_plus_data[(0,1,2),:,:]-0.5)*255 # Normalization\n",
    "        \n",
    "        # annotation\n",
    "        annotation_2D = self.create_annotations(sample_token,self.camera_sensors)\n",
    "        annotation = list()\n",
    "        n=0\n",
    "        for label, box in zip(annotation_2D['labels'], annotation_2D['bboxes']):\n",
    "             if self.class_map[label]>=0:\n",
    "                label = self.class_map[label]\n",
    "                annotation.append( torch.tensor(np.append(box, label)))\n",
    "        if not annotation == []:\n",
    "            annotation = torch.reshape(torch.cat(annotation,dim=0),(-1,5))\n",
    "        else:\n",
    "            annotation =  torch.tensor(annotation)\n",
    "        return image_plus_data, annotation\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_class_label_mapping(category_names, category_mapping):\n",
    "        \"\"\"\n",
    "        :param category_mapping: [dict] Map from original name to target name. Subsets of names are supported. \n",
    "            e.g. {'pedestrian' : 'pedestrian'} will map all pedestrian types to the same label\n",
    "\n",
    "        :returns: \n",
    "            [0]: [dict of (str, int)] mapping from category name to the corresponding index-number\n",
    "            [1]: [dict of (int, str)] mapping from index number to category name\n",
    "        \"\"\"\n",
    "        # Initialize local variables\n",
    "        original_name_to_label = {}\n",
    "        original_category_names = category_names.copy()\n",
    "        original_category_names.append('bg')\n",
    "        if category_mapping is None:\n",
    "            # Create identity mapping and ignore no class\n",
    "            category_mapping = dict()\n",
    "            for cat_name in category_names:\n",
    "                category_mapping[cat_name] = cat_name\n",
    "\n",
    "        # List of unique class_names\n",
    "        selected_category_names = set(category_mapping.values()) # unordered\n",
    "        selected_category_names = list(selected_category_names)\n",
    "        selected_category_names.sort() # ordered\n",
    "      \n",
    "        # Create the label to class_name mapping\n",
    "        label_to_name = { label:name for label, name in enumerate(selected_category_names)}\n",
    "        label_to_name[len(label_to_name)] = 'bg' # Add the background class\n",
    "\n",
    "        # Create original class name to label mapping\n",
    "        for label, label_name in label_to_name.items():\n",
    "\n",
    "            # Looking for all the original names that are adressed by label name\n",
    "            targets = [original_name for original_name in original_category_names if label_name in original_name]\n",
    "\n",
    "            # Assigning the same label for all adressed targets\n",
    "            for target in targets:\n",
    "                \n",
    "                # Check for ambiguity\n",
    "                assert target not in original_name_to_label.keys(), 'ambigous mapping found for (%s->%s)'%(target, label_name)\n",
    "                \n",
    "                # Assign label to original name\n",
    "                # Some label_names will have the same label, which is totally fine\n",
    "                original_name_to_label[target] = label\n",
    "\n",
    "        # Check for correctness\n",
    "        actual_labels = original_name_to_label.values()\n",
    "        expected_labels = range(0, max(actual_labels)+1) # we want to start labels at 0\n",
    "        assert all([label in actual_labels for label in expected_labels]), 'Expected labels do not match actual labels'\n",
    "\n",
    "        return original_name_to_label, label_to_name\n",
    "    \n",
    "    def create_annotations(self, sample_token, sensor_channels):\n",
    "            \"\"\"\n",
    "            Create annotations for the the given sample token.\n",
    "\n",
    "            1 bounding box vector contains:\n",
    "\n",
    "\n",
    "            :param sample_token: the sample_token to get the annotation for\n",
    "            :param sensor_channels: list of channels for cropping the labels, e.g. ['CAM_FRONT', 'RADAR_FRONT']\n",
    "                This works only for CAMERA atm\n",
    "\n",
    "            :returns: \n",
    "                annotations dictionary:\n",
    "                {\n",
    "                    'labels': [] # <list of n int>  \n",
    "                    'bboxes': [] # <list of n x 4 float> [xmin, ymin, xmax, ymax]\n",
    "                    'distances': [] # <list of n float>  Center of box given as x, y, z.\n",
    "                    'visibilities': [] # <list of n float>  Visibility of annotated object\n",
    "                }\n",
    "            \"\"\"\n",
    "\n",
    "            if any([s for s in sensor_channels if 'RADAR' in s]):\n",
    "                print(\"[WARNING] Cropping to RADAR is not supported atm\")\n",
    "                sensor_channels = [c for c in sensor_channels if 'CAM' in sensor_channels]\n",
    "\n",
    "            sample = self.nusc.get('sample', sample_token)\n",
    "            annotations_count = 0\n",
    "            annotations = {\n",
    "                'labels': [], # <list of n int>  \n",
    "                'bboxes': [], # <list of n x 4 float> [xmin, ymin, xmax, ymax]\n",
    "                'distances': [], # <list of n float>  Center of box given as x, y, z.\n",
    "                'visibilities': [],\n",
    "                'num_radar_pts':[] #<list of n int>  number of radar points that cover that annotation\n",
    "                }\n",
    "\n",
    "            # Camera parameters\n",
    "            for selected_sensor_channel in sensor_channels:\n",
    "                sd_rec = self.nusc.get('sample_data', sample['data'][selected_sensor_channel])\n",
    "\n",
    "                # Create Boxes:\n",
    "                _, boxes, camera_intrinsic = self.nusc.get_sample_data(sd_rec['token'], box_vis_level=BoxVisibility.ANY)\n",
    "                imsize_src = (sd_rec['width'], sd_rec['height']) # nuscenes has (width, height) convention\n",
    "                #print(type(boxes[0]))\n",
    "\n",
    "                bbox_resize = [ 1. / sd_rec['height'], 1. / sd_rec['width'] ]\n",
    "                if not self.normalize_bbox:\n",
    "                    bbox_resize[0] *= float(self.image_min_side)\n",
    "                    bbox_resize[1] *= float(self.image_max_side)\n",
    "\n",
    "                # Create labels for all boxes that are visible\n",
    "                for box in boxes:\n",
    "\n",
    "                    # Add labels to boxes \n",
    "                    if box.name in self.classes:\n",
    "                        box.label = self.classes[box.name]\n",
    "                        # Check if box is visible and transform box to 1D vector\n",
    "                        if box_in_image(box=box, intrinsic=camera_intrinsic, imsize=imsize_src, vis_level=BoxVisibility.ANY):\n",
    "\n",
    "                            ## Points in box method for annotation filterS\n",
    "                            # check if bounding box has an according radar point\n",
    "                            if self.only_radar_annotated == 2:\n",
    "\n",
    "                                pcs, times = RadarPointCloud.from_file_multisweep(self.nusc, sample, self.radar_sensors[0], \\\n",
    "                                    selected_sensor_channel, nsweeps=self.n_sweeps, min_distance=0.0, merge=False)\n",
    "\n",
    "                                for pc in pcs:\n",
    "                                    pc.points = radar.enrich_radar_data(pc.points)    \n",
    "\n",
    "                                if len(pcs) > 0:\n",
    "                                    radar_sample = np.concatenate([pc.points for pc in pcs], axis=-1)\n",
    "                                else:\n",
    "                                    print(\"[WARNING] only_radar_annotated=2 and sweeps=0 removes all annotations\")\n",
    "                                    radar_sample = np.zeros(shape=(len(radar.channel_map), 0))\n",
    "                                radar_sample = radar_sample.astype(dtype=np.float32)\n",
    "\n",
    "                                mask = points_in_box(box, radar_sample[0:3,:])\n",
    "                                if True not in mask:\n",
    "                                    continue \n",
    "\n",
    "\n",
    "                            # If visible, we create the corresponding label\n",
    "                            box2d = box.box2d(camera_intrinsic) # returns [xmin, ymin, xmax, ymax]\n",
    "                            box2d[0] *= bbox_resize[1]\n",
    "                            box2d[1] *= bbox_resize[0]\n",
    "                            box2d[2] *= bbox_resize[1]\n",
    "                            box2d[3] *= bbox_resize[0]\n",
    "\n",
    "                            annotations['bboxes'].insert(annotations_count, box2d)\n",
    "                            annotations['labels'].insert(annotations_count, box.label)\n",
    "                            annotations['num_radar_pts'].insert(annotations_count, self.nusc.get('sample_annotation', box.token)['num_radar_pts'])\n",
    "\n",
    "                            distance =  (box.center[0]**2 + box.center[1]**2 + box.center[2]**2)**0.5\n",
    "                            annotations['distances'].insert(annotations_count, distance)\n",
    "                            annotations['visibilities'].insert(annotations_count, int(self.nusc.get('sample_annotation', box.token)['visibility_token']))\n",
    "                            annotations_count += 1\n",
    "                    else:\n",
    "                        # The current name has been ignored\n",
    "                        pass\n",
    "\n",
    "            annotations['labels'] = np.array(annotations['labels'])\n",
    "            annotations['bboxes'] = np.array(annotations['bboxes'])\n",
    "            annotations['distances'] = np.array(annotations['distances'])\n",
    "            annotations['num_radar_pts'] = np.array(annotations['num_radar_pts'])\n",
    "            annotations['visibilities'] = np.array(annotations['visibilities'])\n",
    "\n",
    "            # num_radar_pts mathod for annotation filter\n",
    "            if self.only_radar_annotated == 1:\n",
    "\n",
    "                anns_to_keep = np.where(annotations['num_radar_pts'])[0]\n",
    "\n",
    "                for key in annotations:\n",
    "                    annotations[key] = annotations[key][anns_to_keep]\n",
    "\n",
    "            return annotations\n",
    "    \n",
    "#Data Fusion Functions\n",
    "\n",
    "def _resize_image(image_data, target_shape):\n",
    "    \"\"\"\n",
    "    Perfomrs resizing of the image and calculates a matrix to adapt the intrinsic camera matrix\n",
    "    :param image_data: [np.array] with shape (height x width x 3)\n",
    "    :param target_shape: [tuple] with (width, height)\n",
    "    :return resized image: [np.array] with shape (height x width x 3)\n",
    "    :return resize matrix: [numpy array (3 x 3)]\n",
    "    \"\"\"\n",
    "    # print('resized', type(image_data))\n",
    "    stupid_confusing_cv2_size_because_width_and_height_are_in_wrong_order = (target_shape[1], target_shape[0])\n",
    "    resized_image = cv2.resize(image_data, stupid_confusing_cv2_size_because_width_and_height_are_in_wrong_order)\n",
    "    resize_matrix = np.eye(3, dtype=resized_image.dtype)\n",
    "    resize_matrix[1, 1] = target_shape[0]/image_data.shape[0]\n",
    "    resize_matrix[0, 0] = target_shape[1]/image_data.shape[1]\n",
    "    return resized_image, resize_matrix\n",
    "\n",
    "def _radar_transformation(radar_data, height=None):\n",
    "    \"\"\"\n",
    "    Transforms the given radar data with height z = 0 and another height as input using extrinsic radar matrix to vehicle's co-sy\n",
    "    This function appends the distance to the radar point.\n",
    "    Parameters:\n",
    "    :param radar_data: [numpy array] with radar parameter (e.g. velocity) in rows and radar points for one timestep in columns\n",
    "        Semantics: x y z dyn_prop id rcs vx vy vx_comp vy_comp is_quality_valid ambig_state x_rms y_rms invalid_state pdh0 distance\n",
    "    :param radar_extrinsic: [numpy array (3x4)] that consists of the extrinsic parameters of the given radar sensor\n",
    "    :param height: [tuple] (min height, max height) that defines the (unknown) height of the radar points\n",
    "    Returns:\n",
    "    :returns radar_data: [numpy array (m x no of points)] that consists of the transformed radar points with z = 0\n",
    "    :returns radar_xyz_endpoint: [numpy array (3 x no of points)] that consits of the transformed radar points z = height  \n",
    "    \"\"\"\n",
    "\n",
    "    # Field of view (global)\n",
    "    ELEVATION_FOV_SR = 20\n",
    "    ELEVATION_FOV_FR = 14  \n",
    "\n",
    "    # initialization\n",
    "    num_points = radar_data.shape[1]\n",
    "\n",
    "    # Radar points for the endpoint\n",
    "    radar_xyz_endpoint = radar_data[0:3,:].copy()\n",
    "\n",
    "    # variant 1: constant height substracted by RADAR_HEIGHT\n",
    "    RADAR_HEIGHT = 0.5\n",
    "    if height:\n",
    "        radar_data[2, :] = np.ones((num_points,)) * (height[0] - RADAR_HEIGHT) # lower points\n",
    "        radar_xyz_endpoint[2, :] = np.ones((num_points,)) * (height[1] - RADAR_HEIGHT) # upper points\n",
    "\n",
    "    # variant 2: field of view\n",
    "    else:\n",
    "        dist = radar_data[-1,:]\n",
    "        count = 0\n",
    "        for d in dist:\n",
    "            # short range mode\n",
    "            if d <= 70: \n",
    "                radar_xyz_endpoint[2, count] = -d * np.tan(ELEVATION_FOV_SR/2)\n",
    "\n",
    "            # long range mode\n",
    "            else:\n",
    "                radar_xyz_endpoint[2, count] = -d * np.tan(ELEVATION_FOV_FR/2)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    return radar_data, radar_xyz_endpoint\n",
    "\n",
    "def _create_line(P1, P2, img):\n",
    "    \"\"\"\n",
    "    Produces and array that consists of the coordinates and intensities of each pixel in a line between two points\n",
    "    :param P1: [numpy array] that consists of the coordinate of the first point (x,y)\n",
    "    :param P2: [numpy array] that consists of the coordinate of the second point (x,y)\n",
    "    :param img: [numpy array] the image being processed\n",
    "    :return itbuffer: [numpy array] that consists of the coordinates and intensities of each pixel in the radii (shape: [numPixels, 3], row = [x,y])     \n",
    "    \"\"\"\n",
    "    # define local variables for readability\n",
    "    imageH = img.shape[0]\n",
    "    imageW = img.shape[1]\n",
    "\n",
    "    P1X = P1[0]\n",
    "    P1Y = P1[1]\n",
    "    P2X = P2[0]\n",
    "    P2Y = P2[1]\n",
    "\n",
    "    # difference and absolute difference between points\n",
    "    # used to calculate slope and relative location between points\n",
    "    dX = P2X - P1X\n",
    "    dY = P2Y - P1Y\n",
    "    dXa = np.abs(dX)\n",
    "    dYa = np.abs(dY)\n",
    "\n",
    "    # predefine numpy array for output based on distance between points\n",
    "    itbuffer = np.empty(\n",
    "        shape=(np.maximum(int(dYa), int(dXa)), 2), dtype=np.float32)\n",
    "    itbuffer.fill(np.nan)\n",
    "\n",
    "    # Obtain coordinates along the line using a form of Bresenham's algorithm\n",
    "    negY = P1Y > P2Y\n",
    "    negX = P1X > P2X\n",
    "    if P1X == P2X:  # vertical line segment\n",
    "        itbuffer[:, 0] = P1X\n",
    "        if negY:\n",
    "            itbuffer[:, 1] = np.arange(P1Y - 1, P1Y - dYa - 1, -1)\n",
    "        else:\n",
    "            itbuffer[:, 1] = np.arange(P1Y+1, P1Y+dYa+1)\n",
    "    elif P1Y == P2Y:  # horizontal line segment\n",
    "        itbuffer[:, 1] = P1Y\n",
    "        if negX:\n",
    "            itbuffer[:, 0] = np.arange(P1X-1, P1X-dXa-1, -1)\n",
    "        else:\n",
    "            itbuffer[:, 0] = np.arange(P1X+1, P1X+dXa+1)\n",
    "    else:  # diagonal line segment\n",
    "        steepSlope = dYa > dXa\n",
    "        if steepSlope:\n",
    "            slope = dX.astype(np.float32)/dY.astype(np.float32)\n",
    "            if negY:\n",
    "                itbuffer[:, 1] = np.arange(P1Y-1, P1Y-dYa-1, -1)\n",
    "            else:\n",
    "                itbuffer[:, 1] = np.arange(P1Y+1, P1Y+dYa+1)\n",
    "            itbuffer[:, 0] = (slope*(itbuffer[:, 1]-P1Y)).astype(np.int) + P1X\n",
    "        else:\n",
    "            slope = dY.astype(np.float32)/dX.astype(np.float32)\n",
    "            if negX:\n",
    "                itbuffer[:, 0] = np.arange(P1X-1, P1X-dXa-1, -1)\n",
    "            else:\n",
    "                itbuffer[:, 0] = np.arange(P1X+1, P1X+dXa+1)\n",
    "            itbuffer[:, 1] = (slope*(itbuffer[:, 0]-P1X)).astype(np.int) + P1Y\n",
    "\n",
    "    # Remove points outside of image\n",
    "    colX = itbuffer[:, 0].astype(int)\n",
    "    colY = itbuffer[:, 1].astype(int)\n",
    "    itbuffer = itbuffer[(colX >= 0) & (colY >= 0) &\n",
    "                        (colX < imageW) & (colY < imageH)]\n",
    "\n",
    "    return itbuffer\n",
    "\n",
    "def _create_vertical_line(P1, P2, img):\n",
    "    \"\"\"\n",
    "    Produces and array that consists of the coordinates and intensities of each pixel in a line between two points\n",
    "    :param P1: [numpy array] that consists of the coordinate of the first point (x,y)\n",
    "    :param P2: [numpy array] that consists of the coordinate of the second point (x,y)\n",
    "    :param img: [numpy array] the image being processed\n",
    "    :return itbuffer: [numpy array] that consists of the coordinates and intensities of each pixel in the radii (shape: [numPixels, 3], row = [x,y])     \n",
    "    \"\"\"\n",
    "    # define local variables for readability\n",
    "    imageH = img.shape[0]\n",
    "    imageW = img.shape[1]\n",
    "\n",
    "    # difference and absolute difference between points\n",
    "    # used to calculate slope and relative location between points\n",
    "    P1_y = int(P1[1])\n",
    "    P2_y = int(P2[1])\n",
    "    dX = 0\n",
    "    dY = P2_y - P1_y\n",
    "    if dY == 0:\n",
    "        dY = 1\n",
    "    dXa = np.abs(dX)\n",
    "    dYa = np.abs(dY)\n",
    "\n",
    "    # predefine numpy array for output based on distance between points\n",
    "    itbuffer = np.empty(\n",
    "        shape=(np.maximum(int(dYa), int(dXa)), 2), dtype=np.float32)\n",
    "    itbuffer.fill(np.nan)\n",
    "\n",
    "    # vertical line segment\n",
    "    itbuffer[:, 0] = int(P1[0])\n",
    "    if P1_y > P2_y:\n",
    "        # Obtain coordinates along the line using a form of Bresenham's algorithm\n",
    "        itbuffer[:, 1] = np.arange(P1_y - 1, P1_y - dYa - 1, -1)\n",
    "    else:\n",
    "        itbuffer[:, 1] = np.arange(P1_y+1, P1_y+dYa+1)\n",
    "\n",
    "    # Remove points outside of image\n",
    "    colX = itbuffer[:, 0].astype(int)\n",
    "    colY = itbuffer[:, 1].astype(int)\n",
    "    itbuffer = itbuffer[(colX >= 0) & (colY >= 0) &\n",
    "                        (colX < imageW) & (colY < imageH)]\n",
    "\n",
    "    return itbuffer\n",
    "\n",
    "def _radar2camera(image_data, radar_data, radar_xyz_endpoints, clear_radar=False):\n",
    "    \"\"\"\n",
    "\n",
    "    Calculates a line of two radar points and puts the radar_meta data as additonal layers to the image -> image_plus\n",
    "    :param image_data: [numpy array (900 x 1600 x 3)] of image data\n",
    "    :param radar_data: [numpy array (xyz+meta x no of points)] that consists of the transformed radar points with z = 0\n",
    "        default semantics: x y z dyn_prop id rcs vx vy vx_comp vy_comp is_quality_valid ambig_state x_rms y_rms invalid_state pdh0 vx_rms vy_rms distance\n",
    "    :param radar_xyz_endpoints: [numpy array (3 x no of points)] that consits of the transformed radar points z = height\n",
    "    :param clear_radar: [boolean] True if radar data should be all zero\n",
    "    :return image_plus: a numpy array (900 x 1600 x (3 + number of radar_meta (e.g. velocity)))\n",
    "    \"\"\"\n",
    "\n",
    "    radar_meta_count = radar_data.shape[0]-3\n",
    "    radar_extension = np.zeros(\n",
    "        (image_data.shape[0], image_data.shape[1], radar_meta_count), dtype=np.float32)\n",
    "    no_of_points = radar_data.shape[1]\n",
    "\n",
    "    if clear_radar:\n",
    "        pass # we just don't add it to the image\n",
    "    else:\n",
    "        for radar_point in range(0, no_of_points):\n",
    "            projection_line = _create_vertical_line(\n",
    "                radar_data[0:2, radar_point], radar_xyz_endpoints[0:2, radar_point], image_data)\n",
    "\n",
    "            for pixel_point in range(0, projection_line.shape[0]):\n",
    "                y = projection_line[pixel_point, 1].astype(int)\n",
    "                x = projection_line[pixel_point, 0].astype(int)\n",
    "\n",
    "                # Check if pixel is already filled with radar data and overwrite if distance is less than the existing\n",
    "                if not np.any(radar_extension[y, x]) or radar_data[-1, radar_point] < radar_extension[y, x, -1]:\n",
    "                    radar_extension[y, x] = radar_data[3:, radar_point]\n",
    "\n",
    "\n",
    "    image_plus = np.concatenate((image_data, radar_extension), axis=2)\n",
    "\n",
    "    return image_plus\n",
    "\n",
    "def view_points(points: np.ndarray, view: np.ndarray, normalize: bool):\n",
    "    \"\"\"\n",
    "    This function is a modification of nuscenes.geometry_utils.view_points function\n",
    "    This is a helper class that maps 3d points to a 2d plane. It can be used to implement both perspective and\n",
    "    orthographic projections. It first applies the dot product between the points and the view. By convention,\n",
    "    the view should be such that the data is projected onto the first 2 axis. It then optionally applies a\n",
    "    normalization along the third dimension.\n",
    "    For a perspective projection the view should be a 3x3 camera matrix, and normalize=True\n",
    "    For an orthographic projection with translation the view is a 3x4 matrix and normalize=False\n",
    "    For an orthographic projection without translation the view is a 3x3 matrix (optionally 3x4 with last columns\n",
    "     all zeros) and normalize=False\n",
    "    :param points: <np.float32: 3, n> Matrix of points, where each point (x, y, z) is along each column.\n",
    "    :param view: <np.float32: n, n>. Defines an arbitrary projection (n <= 4).\n",
    "        The projection should be such that the corners are projected onto the first 2 axis.\n",
    "    :param normalize: Whether to normalize the remaining coordinate (along the third axis).\n",
    "    :return: <np.float32: 3, n>. Mapped point. If normalize=False, the third coordinate is the height.\n",
    "    \"\"\"\n",
    "\n",
    "    output = points\n",
    "\n",
    "    assert view.shape[0] <= 4\n",
    "    assert view.shape[1] <= 4\n",
    "    assert points.shape[0] >= 3\n",
    "    points = output[0:3,:]\n",
    "\n",
    "    viewpad = np.eye(4)\n",
    "    viewpad[:view.shape[0], :view.shape[1]] = view\n",
    "\n",
    "    nbr_points = points.shape[1]\n",
    "\n",
    "    # Do operation in homogenous coordinates\n",
    "    points = np.concatenate((points, np.ones((1, nbr_points))))\n",
    "    points = np.dot(viewpad, points)\n",
    "    points = points[:3, :]\n",
    "\n",
    "    if normalize:\n",
    "        points = points / points[2:3, :].repeat(3, 0).reshape(3, nbr_points)\n",
    "\n",
    "    output[0:3,:] = points\n",
    "    return output\n",
    "\n",
    "def map_pointcloud_to_image(nusc, radar_points, pointsensor_token, camera_token, target_resolution=(None,None)):\n",
    "    \"\"\"\n",
    "    Given a point sensor (lidar/radar) token and camera sample_data token, load point-cloud and map it to the image\n",
    "    plane.\n",
    "    :param radar_pints: [list] list of radar points\n",
    "    :param pointsensor_token: [str] Lidar/radar sample_data token.\n",
    "    :param camera_token: [str] Camera sample_data token.\n",
    "    :param target_resolution: [tuple of int] determining the output size for the radar_image. None for no change\n",
    "    :return (points <np.float: 2, n)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the database\n",
    "    cam = nusc.get('sample_data', camera_token)\n",
    "    pointsensor = nusc.get('sample_data', pointsensor_token)\n",
    "\n",
    "    pc = PointCloud(radar_points)\n",
    "\n",
    "    # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n",
    "    # First step: transform the point-cloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "\n",
    "    # Second step: transform to the global frame.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "\n",
    "    # Third step: transform into the ego vehicle frame for the timestamp of the image.\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fourth step: transform into the camera.\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fifth step: actually take a \"picture\" of the point cloud.\n",
    "    # Grab the depths (camera frame z axis points away from the camera).\n",
    "\n",
    "    # intrinsic_resized = np.matmul(camera_resize, np.array(cs_record['camera_intrinsic']))\n",
    "    view = np.array(cs_record['camera_intrinsic'])\n",
    "    # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n",
    "    points = view_points(pc.points, view, normalize=True) #resize here\n",
    "\n",
    "    # Resizing to target resolution\n",
    "    if target_resolution[1]: # resizing width\n",
    "        points[0,:] *= (target_resolution[1]/cam['width'])\n",
    "\n",
    "    if target_resolution[0]: # resizing height\n",
    "        points[1,:] *= (target_resolution[0]/cam['height'])\n",
    "\n",
    "    # actual_resolution = (cam['height'], cam['width'])\n",
    "    # for i in range(len(target_resolution)):\n",
    "    #     if target_resolution[i]:\n",
    "    #         points[i,:] *= (target_resolution[i]/actual_resolution[i])\n",
    "\n",
    "    return points\n",
    "\n",
    "def create_spatial_point_array(nusc, radar_data, pointsensor_token, camera_token, target_width=None):\n",
    "    \"\"\"\n",
    "    This function turns a radar point cloud into a 1-D array by encoding the spatial information.\n",
    "    The position in the array reflects the direction of the radar point with respect to a camera.\n",
    "    :param nusc: [nuscenes.nuscenes.Nuscenes] nuScenes database\n",
    "    :param target_width: [int] the target resolution along x-axis for the output array\n",
    "    :param dim: dimensionality of the target array\n",
    "    \"\"\"\n",
    "    ##########################\n",
    "    ##### Initialization #####\n",
    "    ##########################\n",
    "    radar_meta_count = radar_data.shape[0] - 3 # -3 for substracting the image positions x y z \n",
    "    img_data = nusc.get('sample_data', camera_token)\n",
    "    target_width = target_width or img_data['width']\n",
    "    target_resolution = (1, target_width)\n",
    "    radar_array = np.zeros((*target_resolution, radar_meta_count))\n",
    "\n",
    "    ######################################\n",
    "    ##### Perform the array creation #####\n",
    "    ######################################\n",
    "    # Get radar points with x and y coordinates\n",
    "    projected_radar_points = map_pointcloud_to_image(nusc, radar_data, pointsensor_token=pointsensor_token, \\\n",
    "        camera_token=camera_token, target_resolution=target_resolution)\n",
    "\n",
    "    for i in range(projected_radar_points.shape[1]):\n",
    "        x,y = projected_radar_points[0:2,i].astype(np.int32) # first \n",
    "        if x < 0 or x >= target_width:\n",
    "            continue # we skip this point, because it lies outside of the image\n",
    "        y = 0 # Set height to zero in case the point is outside of the image\n",
    "        radar_array[y,x] = projected_radar_points[3:,i]\n",
    "\n",
    "\n",
    "    ################################\n",
    "    ##### Postprocess the data #####\n",
    "    ################################\n",
    "    # Remove x,y,z from radar data\n",
    "    # radar_array = radar_array[3:,:]\n",
    "\n",
    "    return radar_array\n",
    "\n",
    "def imageplus_creation(nusc, image_data, radar_data, pointsensor_token, camera_token, height=(0,3),image_target_shape=(900, 1600), clear_radar=False, clear_image=False):\n",
    "    \"\"\"\n",
    "    Superordinate function that creates image_plus data of raw camera and radar data\n",
    "    :param nusc: nuScenes initialization\n",
    "    :param image_data: [numpy array] (900 x 1600 x 3)\n",
    "    :param radar_data: [numpy array](SHAPE?) with radar parameter (e.g. velocity) in rows and radar points for one timestep in columns\n",
    "        Semantics:\n",
    "            [0]: x (1)\n",
    "            [1]: y (2)\n",
    "            [2]: z (3)\n",
    "            [3]: dyn_prop (4)\n",
    "            [4]: id (5)\n",
    "            [5]: rcs (6)\n",
    "            [6]: vx (7)\n",
    "            [7]: vy (8)\n",
    "            [8]: vx_comp (9)\n",
    "            [9]: vy_comp (10)\n",
    "            [10]: is_quality_valid (11)\n",
    "            [11]: ambig_state (12)\n",
    "            [12]: x_rms (13)\n",
    "            [13]: y_rms (14)\n",
    "            [14]: invalid_state (15)\n",
    "            [15]: pdh0 (16)\n",
    "            [16]: vx_rms (17)\n",
    "            [17]: vy_rms (18)\n",
    "            [18]: distance (19)\n",
    "    :param pointsensor_token: [str] token of the pointsensor that should be used, most likely radar\n",
    "    :param camera_token: [str] token of the camera sensor\n",
    "    :param height: 2 options for 2 different modi\n",
    "            a.) [tuple] (e.g. height=(0,3)) to define lower and upper boundary\n",
    "            b.) [str] height = 'FOV' for calculating the heights after the field of view of the radar\n",
    "    :param image_target_shape: [tuple] with (height, width), default is (900, 1600)\n",
    "    :param clear_radar: [boolean] True if radar data should be all zero\n",
    "    :param clear_image: [boolean] True if image data should be all zero\n",
    "    :returns: [tuple] image_plus, image\n",
    "        -image_plus: [numpy array] (900 x 1600 x (3 + number of radar_meta (e.g. velocity)))\n",
    "           Semantics:\n",
    "            [0]: R (1)\n",
    "            [1]: G (2)\n",
    "            [2]: B (3)\n",
    "            [3]: dyn_prop (4)\n",
    "            [4]: id (5)\n",
    "            [5]: rcs (6)\n",
    "            [6]: vx (7)\n",
    "            [7]: vy (8)\n",
    "            [8]: vx_comp (9)\n",
    "            [9]: vy_comp (10)\n",
    "            [10]: is_quality_valid (11)\n",
    "            [11]: ambig_state (12)\n",
    "            [12]: x_rms (13)\n",
    "            [13]: y_rms (14)\n",
    "            [14]: invalid_state (15)\n",
    "            [15]: pdh0 (16)\n",
    "            [16]: vx_rms (17)\n",
    "            [17]: vy_rms (18)\n",
    "            [18]: distance (19)\n",
    "        -cur_image: [numpy array] the original, resized image\n",
    "    \"\"\"\n",
    "\n",
    "    ###############################\n",
    "    ##### Preprocess the data #####\n",
    "    ###############################\n",
    "    # enable barcode method\n",
    "    barcode = False\n",
    "    if height[1] > 20:\n",
    "        height = (0,1)\n",
    "        barcode = True\n",
    "\n",
    "    # Resize the image due to a target shape\n",
    "    cur_img, camera_resize = _resize_image(image_data, image_target_shape)\n",
    "\n",
    "    # Get radar points with the desired height and radar meta data\n",
    "    radar_points, radar_xyz_endpoint = _radar_transformation(radar_data, height)\n",
    "\n",
    "    #######################\n",
    "    ##### Filter Data #####\n",
    "    #######################\n",
    "    # Clear the image if clear_image is True\n",
    "    if clear_image: \n",
    "        cur_img.fill(0)\n",
    "\n",
    "    #####################################\n",
    "    ##### Perform the actual Fusion #####\n",
    "    #####################################\n",
    "    # Map the radar points into the image\n",
    "    radar_points = map_pointcloud_to_image(nusc, radar_points, pointsensor_token=pointsensor_token, camera_token=camera_token, target_resolution=image_target_shape)\n",
    "    radar_xyz_endpoint = map_pointcloud_to_image(nusc, radar_xyz_endpoint, pointsensor_token=pointsensor_token, camera_token=camera_token, target_resolution=image_target_shape)\n",
    "\n",
    "    if barcode:\n",
    "        radar_points[1,:] = image_data.shape[0]\n",
    "        radar_xyz_endpoint[1,:] = 0\n",
    "\n",
    "    # Create image plus by creating projection lines and store them as additional channels in the image\n",
    "    image_plus = _radar2camera(cur_img, radar_points, radar_xyz_endpoint, clear_radar=clear_radar)\n",
    "\n",
    "    #########################\n",
    "    ##### Quality Check #####\n",
    "    #########################\n",
    "    # Check if clear_image worked\n",
    "    # if clear_image and np.count_nonzero(image_plus[0:3]):\n",
    "    #     print(\"Clearing image did not work\")\n",
    "\n",
    "    return image_plus\n",
    "\n",
    "def imageplus_creation_camra(image_data, radar_data, calibrator, height=(0,3), \\\n",
    "        image_target_shape=(800, 1280)):\n",
    "\n",
    "    ratio = [image_target_shape[0] / image_data.shape[0], image_target_shape[1] / image_data.shape[1]] \n",
    "    image_data, _  = _resize_image(image_data, image_target_shape)\n",
    "\n",
    "    image_data = image_data/255\n",
    "\n",
    "    x,y,z = radar_data[0:3]\n",
    "\n",
    "    ## Bottom point of projection line\n",
    "    z = np.ones(x.shape) *(height[0]+0.5)\n",
    "\n",
    "    # radar points according to world2cam convention\n",
    "    radar_points = [z,y,-x]\n",
    "    cam_points_low = np.array(calibrator.world2cam(radar_points))\n",
    "    cam_points_low = np.array([ratio[1] * cam_points_low[0], ratio[0] * cam_points_low[1]]).astype(np.uint16)\n",
    "\n",
    "\n",
    "    ## Ceiling point of projection line\n",
    "    z = np.ones(x.shape) *(- height[1] +0.5)\n",
    "\n",
    "    # radar points according to world2cam convention\n",
    "    radar_points = [z,y,-x]\n",
    "    cam_points_high = np.array(calibrator.world2cam(radar_points))\n",
    "    cam_points_high = np.array([ratio[1] * cam_points_high[0], ratio[0] * cam_points_high[1]]).astype(np.uint16)\n",
    "\n",
    "    # Prevent errors in projection where the high point is lower than the low point\n",
    "    points_to_keep = cam_points_high[1,:] < cam_points_low[1,:]\n",
    "    cam_points_high = cam_points_high[:, points_to_keep]\n",
    "    cam_points_low = cam_points_low[:, points_to_keep]\n",
    "\n",
    "\n",
    "    radar_meta_count = radar_data.shape[0]-3\n",
    "    radar_extension = np.zeros((image_data.shape[0], image_data.shape[1], radar_meta_count), dtype=np.float32)\n",
    "    no_of_points = cam_points_low.shape[1]\n",
    "\n",
    "    for radar_point in range(0, no_of_points):\n",
    "        projection_line = _create_vertical_line(\n",
    "            cam_points_low[:, radar_point], cam_points_high[:, radar_point], image_data)\n",
    "\n",
    "        for pixel_point in range(0, projection_line.shape[0]):\n",
    "            y = projection_line[pixel_point, 1].astype(int)\n",
    "            x = projection_line[pixel_point, 0].astype(int)\n",
    "\n",
    "            # Check if pixel is already filled with radar data and overwrite if distance is less than the existing\n",
    "            if not np.any(radar_extension[y, x]) or radar_data[-1, radar_point] < radar_extension[y, x, -1]:\n",
    "                radar_extension[y, x] = radar_data[3:, radar_point]\n",
    "\n",
    "    image_plus = np.concatenate((image_data, radar_extension), axis=2)\n",
    "    return image_plus\n",
    "\n",
    "def create_imagep_visualization(image_plus_data, color_channel=\"distance\", \\\n",
    "        draw_circles=False, cfg=None, radar_lines_opacity=1.0):\n",
    "    \"\"\"\n",
    "    Visualization of image plus data\n",
    "    Parameters:\n",
    "        :image_plus_data: a numpy array (900 x 1600 x (3 + number of radar_meta (e.g. velocity)))\n",
    "        :image_data: a numpy array (900 x 1600 x 3)\n",
    "        :color_channel: <str> Image plus channel for colorizing the radar lines. according to radar.channel_map.\n",
    "        :draw_circles: Draws circles at the bottom of the radar lines\n",
    "    Returns:\n",
    "        :image_data: a numpy array (900 x 1600 x 3)\n",
    "    \"\"\"\n",
    "    # read dimensions\n",
    "    image_plus_height = image_plus_data.shape[0]\n",
    "    image_plus_width = image_plus_data.shape[1]\n",
    "    n_channels = image_plus_data.shape[2]\n",
    "\n",
    "    ##### Extract the image Channels #####\n",
    "    if cfg is None:\n",
    "        image_channels = [0,1,2]\n",
    "    else:\n",
    "        image_channels = [i_ch for i_ch in cfg.channels if i_ch in [0,1,2]]\n",
    "    image_data = np.ones(shape=(*image_plus_data.shape[:2],3))\n",
    "    if len(image_channels) > 0:\n",
    "        image_data[:,:,image_channels] = image_plus_data[:,:,image_channels].copy() # copy so we dont change the old image\n",
    "\n",
    "    # Draw the Horizon\n",
    "    image_data = np.array(image_data*255).astype(np.uint8)\n",
    "    image_data = cv2.cvtColor(image_data, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    ##### Paint every augmented pixel on the image #####\n",
    "    if n_channels > 3:\n",
    "        # transfer it to the currently selected channels\n",
    "        if cfg is None:\n",
    "            print(\"Warning, no cfg provided. Thus, its not possible to find out \\\n",
    "                which channel shall be used for colorization\")\n",
    "            radar_img = np.zeros(image_plus_data.shape[:-1]) # we expect the channel index to be the last axis\n",
    "        else:\n",
    "            available_channels = {radar.channel_map[ch]:ch_idx for ch_idx, ch in enumerate(cfg.channels) if ch > 2}\n",
    "            ch_idx = available_channels[color_channel]\n",
    "            # Normalize the radar\n",
    "            if cfg.normalize_radar: # normalization happens from -127 to 127\n",
    "                radar_img = image_plus_data[...,ch_idx] + 127.5\n",
    "            else:\n",
    "                radar_img = radar.normalize(color_channel, image_plus_data[..., ch_idx],\n",
    "                                            normalization_interval=[0, 255], sigma_factor=2)\n",
    "\n",
    "            radar_img = np.clip(radar_img,0,255)\n",
    "\n",
    "        radar_colormap = np.array(cv2.applyColorMap(radar_img.astype(np.uint8), cv2.COLORMAP_AUTUMN))\n",
    "\n",
    "        for x in range(0, image_plus_width):\n",
    "            for y in range(0, image_plus_height):\n",
    "                radar_channels = image_plus_data[y, x, 3:]\n",
    "                pixel_contains_radar = np.count_nonzero(radar_channels)\n",
    "                if not pixel_contains_radar:\n",
    "                    continue\n",
    "\n",
    "                radar_color = radar_colormap[y,x]\n",
    "                for pixel in [(y,x)]: #[(y,x-1),(y,x),(y,x+1)]:\n",
    "                    if image_data.shape > pixel:\n",
    "\n",
    "                        # Calculate the color\n",
    "                        pixel_color = np.array(image_data[pixel][0:3], dtype=np.uint8)\n",
    "                        pixel_color = np.squeeze(cv2.addWeighted(pixel_color, 1-radar_lines_opacity, radar_color, radar_lines_opacity, 0))\n",
    "\n",
    "                        # Draw on image\n",
    "                        image_data[pixel] = pixel_color\n",
    "\n",
    "                # only if some radar information is there\n",
    "                if draw_circles:\n",
    "                    if image_plus_data.shape[0] > y+1 and not np.any(image_plus_data[y+1, x,3:]):\n",
    "                        cv2.circle(image_data, (x,y), 3, color=radar_colormap[(y,x)].astype(np.float), thickness=1)\n",
    "\n",
    "\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-sight",
   "metadata": {},
   "source": [
    "## CRFNet model\n",
    "The model is based on RetinaNet, which is composed of three main parts:\n",
    "\n",
    "1. Feature Extractor (VGG16)\n",
    "2. Feature Pyramid Network (FPN)\n",
    "3. Regression head and Classification head\n",
    "\n",
    "Different with a classic RetinaNet, CRFNet has the feature map fused with rader signal in every VGG block and FPN output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brilliant-berlin",
   "metadata": {
    "code_folding": [
     0,
     1,
     47
    ]
   },
   "outputs": [],
   "source": [
    "# Constructing the Feature extractor (backbone) class based on VGG16\n",
    "class Backbone(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels = 5):\n",
    "        \n",
    "        super(Backbone, self).__init__()\n",
    "        \n",
    "        self.blocks : List[nn.Sequential] = []\n",
    "        self.in_channels = in_channels\n",
    "        for i in range(5):\n",
    "            blockmodules = []\n",
    "            if i < 4:\n",
    "                blockmodules.append(nn.Conv2d(self.in_channels, (2**i)*64, (3,3), (1,1), (1,1)))\n",
    "                blockmodules.append(nn.ReLU(inplace=True))\n",
    "                blockmodules.append(nn.Conv2d((2**i)*64, (2**i)*64, (3,3), (1,1), (1,1)))\n",
    "                blockmodules.append(nn.ReLU(inplace=True))\n",
    "            \n",
    "            if 4 > i > 1:\n",
    "                blockmodules.append(nn.Conv2d((2**i)*64, (2**i)*64, (3,3), (1,1), (1,1)))\n",
    "                blockmodules.append(nn.ReLU(inplace=True))\n",
    "            \n",
    "            if i == 4:\n",
    "                blockmodules.append(nn.Conv2d(514, 512, (3,3), (1,1), (1,1)))\n",
    "                blockmodules.append(nn.ReLU(inplace=True))\n",
    "                blockmodules.append(nn.Conv2d(512, 512, (3,3), (1,1), (1,1)))\n",
    "                blockmodules.append(nn.ReLU(inplace=True))\n",
    "                blockmodules.append(nn.Conv2d(512, 512, (3,3), (1,1), (1,1)))\n",
    "                blockmodules.append(nn.ReLU(inplace=True))\n",
    "            \n",
    "            blockmodules.append(nn.MaxPool2d(2, 2, 0, 1, ceil_mode=True))\n",
    "            \n",
    "            sequence = nn.Sequential(*blockmodules)\n",
    "            #print(sequence)\n",
    "            self.blocks.append(sequence)\n",
    "            \n",
    "            # plus 2 for radar\n",
    "            self.in_channels = (2**i)*64+2\n",
    "\n",
    "        #print(self.blocks)\n",
    "        self.max_pool = nn.MaxPool2d(2, stride=2,ceil_mode=True)\n",
    "    \n",
    "       \n",
    "        #Initialize dictionary that holds the output data\n",
    "        output_dict = {}\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if not torch.is_tensor(inputs):\n",
    "            inputs = torch.from_numpy(inputs)\n",
    "        output_dict = {}\n",
    "        #Split the camera and radar data\n",
    "        radar_data = inputs[:,3:,:,:]\n",
    "        camera_data = inputs[:,:3,:,:]\n",
    "        #For the first 5 blocks:\n",
    "        for i in range(5):\n",
    "            #Concatenate camera and radar data\n",
    "            combined_data = torch.cat((camera_data, radar_data), 1)\n",
    "            #Apply the max pooling layer to the radar data            \n",
    "            radar_data = self.max_pool(radar_data)\n",
    "            #Apply the VGG16 block to the combined data\n",
    "            camera_data = self.blocks[i](combined_data)\n",
    "            #For blocks 3-5 add the radar and camera data to the output\n",
    "            if i > 1:\n",
    "                vgg_key = \"vgg_output_{}\".format(i+1)\n",
    "                output_dict[vgg_key] = torch.cat((camera_data, radar_data), 1)\n",
    "                rad_key = \"rad_output_{}\".format(i+1)\n",
    "                output_dict[rad_key] = radar_data\n",
    "        \n",
    "        #Apply the final two max pool layers to the radar data and save to output\n",
    "\n",
    "        radar_data = self.max_pool(radar_data)\n",
    "        output_dict['rad_output_6'] = radar_data\n",
    "        \n",
    "        radar_data = self.max_pool(radar_data)\n",
    "        output_dict['rad_output_7'] = radar_data\n",
    "        \n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tired-edmonton",
   "metadata": {
    "code_folding": [
     0,
     2,
     66,
     105
    ]
   },
   "outputs": [],
   "source": [
    "# Constructing the Feature Pyramid Network (FPN) class and the detection heads\n",
    "# FPN\n",
    "class PyramidFeatures(nn.Module):\n",
    "    def __init__(self, C3_size=258, C4_size=514, C5_size=514, feature_size=254):\n",
    "        super(PyramidFeatures, self).__init__()\n",
    "\n",
    "        # upsample C5 to get P5 from the FPN paper\n",
    "        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P5 elementwise to C4\n",
    "        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P4 elementwise to C3\n",
    "        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # \"P6 is obtained via a 3x3 stride-2 conv on C5\"\n",
    "        self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # \"P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6\"\n",
    "        self.P7_1 = nn.ReLU()\n",
    "        self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, inputs_dict, use_radar=True):\n",
    "        C3 = inputs_dict['vgg_output_3'].to(device) \n",
    "        C4 = inputs_dict['vgg_output_4'].to(device)\n",
    "        C5 = inputs_dict['vgg_output_5'].to(device)\n",
    "        R3 = inputs_dict['rad_output_3'].to(device)\n",
    "        R4 = inputs_dict['rad_output_4'].to(device)\n",
    "        R5 = inputs_dict['rad_output_5'].to(device)\n",
    "        R6 = inputs_dict['rad_output_6'].to(device)\n",
    "        R7 = inputs_dict['rad_output_7'].to(device)\n",
    "\n",
    "        P5_x = self.P5_1(C5)\n",
    "        P5_x = self.P5_2(P5_x)\n",
    "\n",
    "        P4_x = self.P4_1(C4)\n",
    "        P5_upsampled_x = F.interpolate(P5_x,size=[P4_x.shape[2],P4_x.shape[3]], mode='nearest')\n",
    "        P4_x = P5_upsampled_x + P4_x\n",
    "        P4_x = self.P4_2(P4_x)\n",
    "        \n",
    "        P3_x = self.P3_1(C3)\n",
    "        P4_upsampled_x = F.interpolate(P4_x,size=[P3_x.shape[2],P3_x.shape[3]], mode='nearest')\n",
    "        P3_x = P3_x + P4_upsampled_x\n",
    "        P3_x = self.P3_2(P3_x)\n",
    "\n",
    "        P6_x = self.P6(C5)\n",
    "\n",
    "        P7_x = self.P7_1(P6_x)\n",
    "        P7_x = self.P7_2(P7_x)\n",
    "       \n",
    "        #print(P3_x.shape,P4_x.shape,P5_x.shape,P6_x.shape,P7_x.shape)\n",
    "        #print(R3.shape,R4.shape,R5.shape,R6.shape,R7.shape)\n",
    "        if use_radar:\n",
    "            P3_x = torch.cat((P3_x, R3),1)\n",
    "            P4_x = torch.cat((P4_x, R4),1)\n",
    "            P5_x = torch.cat((P5_x, R5),1)\n",
    "            P6_x = torch.cat((P6_x, R6),1)\n",
    "            P7_x = torch.cat((P7_x, R7),1)\n",
    "            return [P3_x, P4_x, P5_x, P6_x, P7_x]\n",
    "        else:         \n",
    "            return [P3_x, P4_x, P5_x, P6_x, P7_x]\n",
    "\n",
    "# Detection Heads\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, num_features_in=256, num_anchors=9, feature_size=256):\n",
    "        super(RegressionModel, self).__init__()\n",
    "\n",
    "        # num_features_in: channels of [P3_x, P4_x, P5_x, P6_x, P7_x]\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "\n",
    "        # out is B x C x W x H, with C = 4*num_anchors\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        return out.contiguous().view(out.shape[0], -1, 4)   \n",
    "    \n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_features_in=256, num_anchors=9, num_classes=8, prior=0.01, feature_size=256):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1)\n",
    "        self.output_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "        out = self.output_act(out)\n",
    "\n",
    "        # out is B x C x W x H, with C = n_classes + n_anchors\n",
    "        out1 = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        batch_size, width, height, channels = out1.shape\n",
    "\n",
    "        out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)\n",
    "\n",
    "        return out2.contiguous().view(x.shape[0], -1, self.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "moved-photographer",
   "metadata": {
    "code_folding": [
     0,
     1,
     2,
     32,
     55
    ]
   },
   "outputs": [],
   "source": [
    "# Constructing the Anchors class to generate anchors for features                                                      \n",
    "class RetinaAnchors(nn.Module):\n",
    "    def __init__(self, areas, ratios, scales, strides):\n",
    "        super(RetinaAnchors, self).__init__()\n",
    "        self.areas = areas\n",
    "        self.ratios = ratios\n",
    "        self.scales = scales\n",
    "        self.strides = strides\n",
    "\n",
    "    def forward(self, batch_size, fpn_feature_sizes):\n",
    "        \"\"\"\n",
    "        generate batch anchors\n",
    "        \"\"\"\n",
    "        device = fpn_feature_sizes.device\n",
    "        one_sample_anchors = []\n",
    "        for index, area in enumerate(self.areas):\n",
    "            base_anchors = self.generate_base_anchors(area, self.scales,self.ratios)\n",
    "            featrue_anchors = self.generate_anchors_on_feature_map(base_anchors, fpn_feature_sizes[index], self.strides[index])\n",
    "            featrue_anchors = featrue_anchors.to(device)\n",
    "            one_sample_anchors.append(featrue_anchors)\n",
    "\n",
    "        batch_anchors = []\n",
    "        for per_level_featrue_anchors in one_sample_anchors:\n",
    "            per_level_featrue_anchors = per_level_featrue_anchors.unsqueeze(\n",
    "                0).repeat(batch_size, 1, 1)\n",
    "            batch_anchors.append(per_level_featrue_anchors)\n",
    "\n",
    "        # if input size:[B,3,640,640]\n",
    "        # batch_anchors shape:[[B, 57600, 4],[B, 14400, 4],[B, 3600, 4],[B, 900, 4],[B, 225, 4]]\n",
    "        # per anchor format:[x_min,y_min,x_max,y_max]\n",
    "        return batch_anchors\n",
    "\n",
    "    def generate_base_anchors(self, area, scales, ratios):\n",
    "        \"\"\"\n",
    "        generate base anchor\n",
    "        \"\"\"\n",
    "        # get w,h aspect ratio,shape:[9,2]\n",
    "        aspects = torch.tensor([[[s * math.sqrt(r), s * math.sqrt(1 / r)]\n",
    "                                 for s in scales]\n",
    "                                for r in ratios]).view(-1, 2)\n",
    "        # base anchor for each position on feature map,shape[9,4]\n",
    "        base_anchors = torch.zeros((len(scales) * len(ratios), 4))\n",
    "\n",
    "        # compute aspect w\\h,shape[9,2]\n",
    "        base_w_h = area * aspects\n",
    "        base_anchors[:, 2:] += base_w_h\n",
    "\n",
    "        # base_anchors format: [x_min,y_min,x_max,y_max],center point:[0,0],shape[9,4]\n",
    "        base_anchors[:, 0] -= base_anchors[:, 2] / 2\n",
    "        base_anchors[:, 1] -= base_anchors[:, 3] / 2\n",
    "        base_anchors[:, 2] /= 2\n",
    "        base_anchors[:, 3] /= 2\n",
    "\n",
    "        return base_anchors\n",
    "\n",
    "    def generate_anchors_on_feature_map(self, base_anchors, feature_map_size,\n",
    "                                        stride):\n",
    "        \"\"\"\n",
    "        generate all anchors on a feature map\n",
    "        \"\"\"\n",
    "        # shifts_x shape:[w],shifts_x shape:[h]\n",
    "        shifts_x = (torch.arange(0, feature_map_size[0]) + 0.5) * stride\n",
    "        shifts_y = (torch.arange(0, feature_map_size[1]) + 0.5) * stride\n",
    "\n",
    "        # shifts shape:[w,h,2] -> [w,h,4] -> [w,h,1,4]\n",
    "        shifts = torch.tensor([[[shift_x, shift_y] for shift_y in shifts_y]\n",
    "                               for shift_x in shifts_x]).repeat(1, 1,\n",
    "                                                                2).unsqueeze(2)\n",
    "\n",
    "        # base anchors shape:[9,4] -> [1,1,9,4]\n",
    "        base_anchors = base_anchors.unsqueeze(0).unsqueeze(0)\n",
    "        # generate all featrue map anchors on each feature map points\n",
    "        # featrue map anchors shape:[w,h,9,4] -> [h,w,9,4] -> [h*w*9,4]\n",
    "        feature_map_anchors = (base_anchors + shifts).permute(\n",
    "            1, 0, 2, 3).contiguous().view(-1, 4)\n",
    "\n",
    "        # feature_map_anchors format: [anchor_nums,4],4:[x_min,y_min,x_max,y_max]\n",
    "        return feature_map_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "elementary-peninsula",
   "metadata": {
    "code_folding": [
     0,
     1,
     2,
     18
    ]
   },
   "outputs": [],
   "source": [
    "# Constructing the CRF net class, which combines the networks \n",
    "class CRFNet(nn.Module):\n",
    "    def __init__(self,num_class=8):\n",
    "        super(CRFNet, self).__init__()\n",
    "        \n",
    "        self.backbone = Backbone().to(device) \n",
    "        self.fpn = PyramidFeatures().to(device)\n",
    "        self.regression = RegressionModel().to(device)\n",
    "        self.distance = RegressionModel().to(device)\n",
    "        self.classification = ClassificationModel().to(device)\n",
    "        \n",
    "        # Setting anchors\n",
    "        self.areas = torch.tensor([[16, 16], [32, 32], [64, 64], [128, 128], [256, 256]])\n",
    "        self.ratios = torch.tensor([0.5, 1, 2])\n",
    "        self.scales = torch.tensor([2**0, 2**(1.0 / 3.0), 2**(2.0 / 3.0)])\n",
    "        self.strides = torch.tensor([8, 16, 32, 64, 128], dtype=torch.float)\n",
    "        self.anchors = RetinaAnchors(self.areas, self.ratios, self.scales, self.strides)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.batch_size, _, _, _ = inputs.shape\n",
    "        device = inputs.device\n",
    "        \n",
    "        # Backbone, input:image+radar, output: C3, C4, C5, R3, R4, R5, R6, R7 \n",
    "        backbone_output = self.backbone.forward(inputs)\n",
    "        \n",
    "        # FPN, input: C3, C4, C5, R3, R4, R5, R6, R7 , output: P3, P4, P5, P6, P7 (features)\n",
    "        features = self.fpn.forward(backbone_output)\n",
    "        \n",
    "        # Detection Head, input: P4, P5, P6, P7, output:\n",
    "        regression = torch.cat([self.regression.forward(feature) for feature in features], dim=1)\n",
    "        classification = torch.cat([self.classification.forward(feature) for feature in features], dim=1)\n",
    "        distance = torch.cat([self.distance.forward(feature) for feature in features], dim=1)\n",
    "        \n",
    "        # Anchors\n",
    "        self.fpn_feature_sizes = []\n",
    "        for feature in features:\n",
    "            self.fpn_feature_sizes.append([feature.shape[3], feature.shape[2]])\n",
    "        self.fpn_feature_sizes = torch.tensor(self.fpn_feature_sizes).to(device)\n",
    "        #print(self.fpn_feature_sizes)\n",
    "        batch_anchors = self.anchors(self.batch_size, self.fpn_feature_sizes)\n",
    "        batch_anchors = torch.cat([batch for batch in batch_anchors],dim=1)\n",
    "        \n",
    "        return regression, classification, distance, batch_anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-poetry",
   "metadata": {},
   "source": [
    "## Loss and prediction\n",
    "Besides the model itself, several other components need to be implemented \n",
    "\n",
    "1. The focal loss function for classification head result and smoothe-L1 loss for regression head output\n",
    "2. The decoder based on NMS(non-maximum supression), which transform the output of detection heads to predication boxes\n",
    "3. The metrix used to measure the predication performance: mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proof-rwanda",
   "metadata": {
    "code_folding": [
     0,
     1,
     2,
     17,
     61,
     101,
     131,
     184,
     236,
     261
    ]
   },
   "outputs": [],
   "source": [
    "# Construting the focal loss and regression loss functions \n",
    "class RetinaLoss(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_w,\n",
    "                 image_h,\n",
    "                 alpha=0.25,\n",
    "                 gamma=2,\n",
    "                 beta=1.0 / 9.0,\n",
    "                 epsilon=1e-4):\n",
    "        super(RetinaLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.image_w = image_w\n",
    "        self.image_h = image_h\n",
    "\n",
    "    def forward(self, cls_heads, reg_heads, batch_anchors, annotations):\n",
    "        \"\"\"\n",
    "        compute cls loss and reg loss in one batch\n",
    "        \"\"\"\n",
    "        #device = annotations.device\n",
    "        #cls_heads = torch.cat(cls_heads, axis=1)\n",
    "        #reg_heads = torch.cat(reg_heads, axis=1)\n",
    "        #batch_anchors = torch.cat(batch_anchors, axis=1)\n",
    "\n",
    "        cls_heads, reg_heads, batch_anchors = self.drop_out_border_anchors_and_heads(\n",
    "            cls_heads, reg_heads, batch_anchors, self.image_w, self.image_h)\n",
    "        #print(cls_heads.shape, reg_heads.shape, batch_anchors.shape)\n",
    "        batch_anchors_annotations = self.get_batch_anchors_annotations(batch_anchors, annotations)\n",
    "        #print(batch_anchors_annotations.shape)\n",
    "\n",
    "        cls_loss, reg_loss = [], []\n",
    "        valid_image_num = 0\n",
    "        for per_image_cls_heads, per_image_reg_heads, per_image_anchors_annotations in zip(\n",
    "                cls_heads, reg_heads, batch_anchors_annotations):\n",
    "            # valid anchors contain all positive anchors\n",
    "            valid_anchors_num = (per_image_anchors_annotations[\n",
    "                per_image_anchors_annotations[:, 4] > 0]).shape[0]\n",
    "            #print(\"valid_anchors_num:\", valid_anchors_num)\n",
    "            #print(per_image_anchors_annotations[per_image_anchors_annotations[:, 4] == 0].shape)\n",
    "            if valid_anchors_num == 0:\n",
    "                cls_loss.append(torch.tensor(0.).to(device))\n",
    "                reg_loss.append(torch.tensor(0.).to(device))\n",
    "            else:\n",
    "                valid_image_num += 1\n",
    "                one_image_cls_loss = self.compute_one_image_focal_loss(\n",
    "                    per_image_cls_heads, per_image_anchors_annotations)\n",
    "                one_image_reg_loss = self.compute_one_image_smoothl1_loss(\n",
    "                    per_image_reg_heads, per_image_anchors_annotations)\n",
    "                cls_loss.append(one_image_cls_loss)\n",
    "                reg_loss.append(one_image_reg_loss)\n",
    "        \n",
    "        if valid_image_num==0:\n",
    "            return cls_loss[0], reg_loss[0]\n",
    "        \n",
    "        cls_loss = sum(cls_loss) / valid_image_num\n",
    "        reg_loss = sum(reg_loss) / valid_image_num\n",
    "\n",
    "        return cls_loss, reg_loss\n",
    "\n",
    "    def compute_one_image_focal_loss(self, per_image_cls_heads,\n",
    "                                     per_image_anchors_annotations):\n",
    "        \"\"\"\n",
    "        compute one image focal loss(cls loss)\n",
    "        per_image_cls_heads:[anchor_num,num_classes]\n",
    "        per_image_anchors_annotations:[anchor_num,5]\n",
    "        \"\"\"\n",
    "        # Filter anchors with gt class=-1, this part of anchor doesn't calculate focal loss\n",
    "        per_image_cls_heads = per_image_cls_heads[per_image_anchors_annotations[:, 4] >= 0]\n",
    "           \n",
    "        per_image_anchors_annotations = per_image_anchors_annotations[per_image_anchors_annotations[:, 4] >= 0]\n",
    "            \n",
    "        per_image_cls_heads = torch.clamp(per_image_cls_heads,\n",
    "                                          min=self.epsilon,\n",
    "                                          max=1. - self.epsilon)\n",
    "        num_classes = per_image_cls_heads.shape[1]\n",
    "\n",
    "        # generate 8 binary ground truth classes for each anchor\n",
    "        loss_ground_truth = F.one_hot(per_image_anchors_annotations[:,4].long(),num_classes=num_classes + 1)\n",
    "        loss_ground_truth = loss_ground_truth[:, 1:]\n",
    "        loss_ground_truth = loss_ground_truth.float().to(device)\n",
    "\n",
    "        alpha_factor = torch.ones_like(per_image_cls_heads) * self.alpha\n",
    "        alpha_factor = torch.where(torch.eq(loss_ground_truth, 1.),alpha_factor, 1. - alpha_factor)\n",
    "        \n",
    "        pt = torch.where(torch.eq(loss_ground_truth, 1.), per_image_cls_heads, 1. - per_image_cls_heads)\n",
    "        focal_weight = alpha_factor * torch.pow((1. - pt), self.gamma)\n",
    "\n",
    "        bce_loss = -(loss_ground_truth * torch.log(per_image_cls_heads) +(1. - loss_ground_truth) * torch.log(1. - per_image_cls_heads))\n",
    "\n",
    "        one_image_focal_loss = focal_weight * bce_loss\n",
    "\n",
    "        one_image_focal_loss = one_image_focal_loss.sum()\n",
    "        positive_anchors_num = per_image_anchors_annotations[per_image_anchors_annotations[:, 4] > 0].shape[0]\n",
    "        \n",
    "        # according to the original paper,We divide the focal loss by the number of positive sample anchors\n",
    "        one_image_focal_loss = one_image_focal_loss / positive_anchors_num\n",
    "\n",
    "        return one_image_focal_loss\n",
    "\n",
    "    def compute_one_image_smoothl1_loss(self, per_image_reg_heads,\n",
    "                                        per_image_anchors_annotations):\n",
    "        \"\"\"\n",
    "        compute one image smoothl1 loss(reg loss)\n",
    "        per_image_reg_heads:[anchor_num,4]\n",
    "        per_image_anchors_annotations:[anchor_num,5]\n",
    "        \"\"\"\n",
    "        # Filter anchors with gt class=-1, this part of anchor doesn't calculate smoothl1 loss\n",
    "        #device = per_image_reg_heads.device\n",
    "        per_image_reg_heads = per_image_reg_heads[\n",
    "            per_image_anchors_annotations[:, 4] > 0]\n",
    "        per_image_anchors_annotations = per_image_anchors_annotations[\n",
    "            per_image_anchors_annotations[:, 4] > 0]\n",
    "        positive_anchor_num = per_image_anchors_annotations.shape[0]\n",
    "\n",
    "        if positive_anchor_num == 0:\n",
    "            return torch.tensor(0.).to(device)\n",
    "\n",
    "        # compute smoothl1 loss\n",
    "        loss_ground_truth = per_image_anchors_annotations[:, 0:4]\n",
    "        x = torch.abs(per_image_reg_heads - loss_ground_truth)\n",
    "        one_image_smoothl1_loss = torch.where(torch.ge(x, self.beta),\n",
    "                                              x - 0.5 * self.beta,\n",
    "                                              0.5 * (x**2) / self.beta)\n",
    "        one_image_smoothl1_loss = one_image_smoothl1_loss.mean(axis=1).sum()\n",
    "        # according to the original paper,We divide the smoothl1 loss by the number of positive sample anchors\n",
    "        one_image_smoothl1_loss = one_image_smoothl1_loss / positive_anchor_num\n",
    "\n",
    "        return one_image_smoothl1_loss\n",
    "\n",
    "    def drop_out_border_anchors_and_heads(self, cls_heads, reg_heads,\n",
    "                                          batch_anchors, image_w, image_h):\n",
    "        \"\"\"\n",
    "        dropout out of border anchors,cls heads and reg heads\n",
    "        \"\"\"\n",
    "        final_cls_heads, final_reg_heads, final_batch_anchors = [], [], []\n",
    "        for per_image_cls_head, per_image_reg_head, per_image_anchors in zip(\n",
    "                cls_heads, reg_heads, batch_anchors):\n",
    "            per_image_cls_head = per_image_cls_head[per_image_anchors[:,\n",
    "                                                                      0] > 0.0]\n",
    "            per_image_reg_head = per_image_reg_head[per_image_anchors[:,\n",
    "                                                                      0] > 0.0]\n",
    "            per_image_anchors = per_image_anchors[per_image_anchors[:,\n",
    "                                                                    0] > 0.0]\n",
    "\n",
    "            per_image_cls_head = per_image_cls_head[per_image_anchors[:,\n",
    "                                                                      1] > 0.0]\n",
    "            per_image_reg_head = per_image_reg_head[per_image_anchors[:,\n",
    "                                                                      1] > 0.0]\n",
    "            per_image_anchors = per_image_anchors[per_image_anchors[:,\n",
    "                                                                    1] > 0.0]\n",
    "\n",
    "            per_image_cls_head = per_image_cls_head[\n",
    "                per_image_anchors[:, 2] < image_w]\n",
    "            per_image_reg_head = per_image_reg_head[\n",
    "                per_image_anchors[:, 2] < image_w]\n",
    "            per_image_anchors = per_image_anchors[\n",
    "                per_image_anchors[:, 2] < image_w]\n",
    "\n",
    "            per_image_cls_head = per_image_cls_head[\n",
    "                per_image_anchors[:, 3] < image_h]\n",
    "            per_image_reg_head = per_image_reg_head[\n",
    "                per_image_anchors[:, 3] < image_h]\n",
    "            per_image_anchors = per_image_anchors[\n",
    "                per_image_anchors[:, 3] < image_h]\n",
    "\n",
    "            per_image_cls_head = per_image_cls_head.unsqueeze(0)\n",
    "            per_image_reg_head = per_image_reg_head.unsqueeze(0)\n",
    "            per_image_anchors = per_image_anchors.unsqueeze(0)\n",
    "\n",
    "            final_cls_heads.append(per_image_cls_head)\n",
    "            final_reg_heads.append(per_image_reg_head)\n",
    "            final_batch_anchors.append(per_image_anchors)\n",
    "\n",
    "        final_cls_heads = torch.cat(final_cls_heads, axis=0)\n",
    "        final_reg_heads = torch.cat(final_reg_heads, axis=0)\n",
    "        final_batch_anchors = torch.cat(final_batch_anchors, axis=0)\n",
    "\n",
    "        # final cls heads shape:[batch_size, anchor_nums, class_num]\n",
    "        # final reg heads shape:[batch_size, anchor_nums, 4]\n",
    "        # final batch anchors shape:[batch_size, anchor_nums, 4]\n",
    "        return final_cls_heads, final_reg_heads, final_batch_anchors\n",
    "\n",
    "    def get_batch_anchors_annotations(self, batch_anchors, annotations):\n",
    "        \"\"\"\n",
    "        Assign a ground truth box target and a ground truth class target for each anchor\n",
    "        if anchor gt_class index = -1,this anchor doesn't calculate cls loss and reg loss\n",
    "        if anchor gt_class index = 0,this anchor is a background class anchor and used in calculate cls loss\n",
    "        if anchor gt_class index > 0,this anchor is a object class anchor and used in calculate cls loss and reg loss\n",
    "        \"\"\"\n",
    "        #device = annotations.device\n",
    "        #assert batch_anchors.shape[0] == annotations.shape[0]\n",
    "        one_image_anchor_nums = batch_anchors.shape[1]\n",
    "        batch_anchors_annotations = []\n",
    "        for one_image_anchors, one_image_annotations in zip(\n",
    "                batch_anchors, annotations):\n",
    "            # drop all index=-1 class annotations\n",
    "            one_image_annotations = one_image_annotations[\n",
    "                one_image_annotations[:, 4] >= 0]\n",
    "            #print(one_image_annotations.shape)\n",
    "\n",
    "            if one_image_annotations.shape[0] == 0:\n",
    "                one_image_anchor_annotations = torch.ones(\n",
    "                    [one_image_anchor_nums, 5], device=device) * (-1)\n",
    "            else:\n",
    "                one_image_gt_bboxes = one_image_annotations[:, 0:4]\n",
    "                one_image_gt_class = one_image_annotations[:, 4].to(device)\n",
    "                one_image_ious = self.compute_ious_for_one_image(one_image_anchors, one_image_gt_bboxes)\n",
    "\n",
    "                # snap per gt bboxes to the best iou anchor\n",
    "                overlap, indices = one_image_ious.max(axis=1)\n",
    "                # assgin each anchor gt bboxes for max iou annotation\n",
    "                per_image_anchors_gt_bboxes = one_image_gt_bboxes[indices]\n",
    "                # transform gt bboxes to [tx,ty,tw,th] format for each anchor\n",
    "                one_image_anchors_snaped_boxes = self.snap_annotations_as_tx_ty_tw_th(\n",
    "                    per_image_anchors_gt_bboxes, one_image_anchors)\n",
    "\n",
    "                one_image_anchors_gt_class = (torch.ones_like(overlap) *-1).to(device)\n",
    "                # if iou <0.4,assign anchors gt class as 0:background\n",
    "                one_image_anchors_gt_class[overlap < 0.4] = 0\n",
    "                # if iou >=0.5,assign anchors gt class as same as the max iou annotation class:80 classes index from 1 to 80\n",
    "                one_image_anchors_gt_class[overlap >=0.5] = one_image_gt_class[indices][overlap >= 0.5] + 1\n",
    "\n",
    "                one_image_anchors_gt_class = one_image_anchors_gt_class.unsqueeze(-1)\n",
    "\n",
    "                one_image_anchor_annotations = torch.cat([one_image_anchors_snaped_boxes, one_image_anchors_gt_class], axis=1)\n",
    "            \n",
    "            one_image_anchor_annotations = one_image_anchor_annotations.unsqueeze(0)\n",
    "            batch_anchors_annotations.append(one_image_anchor_annotations)\n",
    "\n",
    "        batch_anchors_annotations = torch.cat(batch_anchors_annotations,axis=0)\n",
    "\n",
    "        # batch anchors annotations shape:[batch_size, anchor_nums, 5]\n",
    "        return batch_anchors_annotations\n",
    "\n",
    "    def snap_annotations_as_tx_ty_tw_th(self, anchors_gt_bboxes, anchors):\n",
    "        \"\"\"\n",
    "        snap each anchor ground truth bbox form format:[x_min,y_min,x_max,y_max] to format:[tx,ty,tw,th]\n",
    "        \"\"\"\n",
    "        anchors_w_h = anchors[:, 2:] - anchors[:, :2]\n",
    "        anchors_ctr = anchors[:, :2] + 0.5 * anchors_w_h\n",
    "\n",
    "        anchors_gt_bboxes_w_h = anchors_gt_bboxes[:,\n",
    "                                                  2:] - anchors_gt_bboxes[:, :2]\n",
    "        anchors_gt_bboxes_w_h = torch.clamp(anchors_gt_bboxes_w_h, min=1.0)\n",
    "        anchors_gt_bboxes_ctr = anchors_gt_bboxes[:, :\n",
    "                                                  2] + 0.5 * anchors_gt_bboxes_w_h\n",
    "\n",
    "        snaped_annotations_for_anchors = torch.cat(\n",
    "            [(anchors_gt_bboxes_ctr - anchors_ctr) / anchors_w_h,\n",
    "             torch.log(anchors_gt_bboxes_w_h / anchors_w_h)],\n",
    "            axis=1).to(device)\n",
    "        #device = snaped_annotations_for_anchors.device\n",
    "        factor = torch.tensor([[0.1, 0.1, 0.2, 0.2]]).to(device)\n",
    "\n",
    "        snaped_annotations_for_anchors = snaped_annotations_for_anchors / factor\n",
    "\n",
    "        # snaped_annotations_for_anchors shape:[batch_size, anchor_nums, 4]\n",
    "        return snaped_annotations_for_anchors\n",
    "\n",
    "    def compute_ious_for_one_image(self, one_image_anchors,\n",
    "                                   one_image_annotations):\n",
    "        \"\"\"\n",
    "        compute ious between one image anchors and one image annotations\n",
    "        \"\"\"\n",
    "        # make sure anchors format:[anchor_nums,4],4:[x_min,y_min,x_max,y_max]\n",
    "        # make sure annotations format: [annotation_nums,4],4:[x_min,y_min,x_max,y_max]\n",
    "        annotation_num = one_image_annotations.shape[0]\n",
    "\n",
    "        one_image_ious = []\n",
    "        for annotation_index in range(annotation_num):\n",
    "            annotation = one_image_annotations[\n",
    "                annotation_index:annotation_index + 1, :]\n",
    "            overlap_area_top_left = torch.max(one_image_anchors[:, :2],\n",
    "                                              annotation[:, :2])\n",
    "            overlap_area_bot_right = torch.min(one_image_anchors[:, 2:],\n",
    "                                               annotation[:, 2:])\n",
    "            overlap_area_sizes = torch.clamp(overlap_area_bot_right -\n",
    "                                             overlap_area_top_left,\n",
    "                                             min=0)\n",
    "            overlap_area = overlap_area_sizes[:, 0] * overlap_area_sizes[:, 1]\n",
    "            # anchors and annotations convert format to [x1,y1,w,h]\n",
    "            anchors_w_h = one_image_anchors[:,\n",
    "                                            2:] - one_image_anchors[:, :2] + 1\n",
    "            annotations_w_h = annotation[:, 2:] - annotation[:, :2] + 1\n",
    "            # compute anchors_area and annotations_area\n",
    "            anchors_area = anchors_w_h[:, 0] * anchors_w_h[:, 1]\n",
    "            annotations_area = annotations_w_h[:, 0] * annotations_w_h[:, 1]\n",
    "\n",
    "            # compute union_area\n",
    "            union_area = anchors_area + annotations_area - overlap_area\n",
    "            union_area = torch.clamp(union_area, min=1e-4)\n",
    "            # compute ious between one image anchors and one image annotations\n",
    "            ious = (overlap_area / union_area).unsqueeze(-1)\n",
    "\n",
    "            one_image_ious.append(ious)\n",
    "\n",
    "        one_image_ious = torch.cat(one_image_ious, axis=1)\n",
    "\n",
    "        # one image ious shape:[anchors_num,annotation_num]\n",
    "        return one_image_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "catholic-junction",
   "metadata": {
    "code_folding": [
     0,
     1,
     2,
     13,
     64,
     80,
     141
    ]
   },
   "outputs": [],
   "source": [
    "# Constructing the decoder for transforming the output to prediction\n",
    "class RetinaDecoder(nn.Module):\n",
    "    def __init__(self,image_w,image_h,\n",
    "                 min_score_threshold=0.05,\n",
    "                 nms_threshold=0.5,\n",
    "                 max_detection_num=300):\n",
    "        super(RetinaDecoder, self).__init__()\n",
    "        self.image_w = image_w\n",
    "        self.image_h = image_h\n",
    "        self.min_score_threshold = min_score_threshold\n",
    "        self.nms_threshold = nms_threshold\n",
    "        self.max_detection_num = max_detection_num\n",
    "\n",
    "    def forward(self, cls_heads, reg_heads, batch_anchors):\n",
    "        with torch.no_grad():\n",
    "            device = cls_heads[0].device\n",
    "\n",
    "            batch_scores, batch_classes, batch_pred_bboxes = [], [], []\n",
    "            for per_image_cls_heads, per_image_reg_heads, per_image_anchors in zip(cls_heads, reg_heads, batch_anchors):\n",
    "                # transfer reg head\n",
    "                pred_bboxes = self.snap_tx_ty_tw_th_reg_heads_to_x1_y1_x2_y2_bboxes(per_image_reg_heads, per_image_anchors)\n",
    "               \n",
    "                # score of classification heads \n",
    "                scores, score_classes = torch.max(per_image_cls_heads, dim=1)\n",
    "                score_classes = score_classes[scores > self.min_score_threshold].float()    \n",
    "                pred_bboxes = pred_bboxes[ scores > self.min_score_threshold].float()    \n",
    "                scores = scores[scores > self.min_score_threshold].float()\n",
    "\n",
    "                single_image_scores = (-1) * torch.ones((self.max_detection_num, ), device=device)\n",
    "                single_image_classes = (-1) * torch.ones((self.max_detection_num, ), device=device)\n",
    "                single_image_pred_bboxes = (-1) * torch.ones((self.max_detection_num, 4), device=device)\n",
    "\n",
    "                if scores.shape[0] != 0:\n",
    "                    scores, score_classes, pred_bboxes = self.nms(scores, score_classes, pred_bboxes)\n",
    "                    #print(scores)\n",
    "                    sorted_keep_scores, sorted_keep_scores_indexes = torch.sort(scores, descending=True)\n",
    "                    sorted_keep_classes = score_classes[sorted_keep_scores_indexes]\n",
    "                    sorted_keep_pred_bboxes = pred_bboxes[sorted_keep_scores_indexes]\n",
    "\n",
    "                    final_detection_num = min(self.max_detection_num,sorted_keep_scores.shape[0])\n",
    "\n",
    "                    single_image_scores[0:final_detection_num] = sorted_keep_scores[0:final_detection_num]\n",
    "                    single_image_classes[0:final_detection_num] = sorted_keep_classes[0:final_detection_num]\n",
    "                    single_image_pred_bboxes[0:final_detection_num, :] = sorted_keep_pred_bboxes[0:final_detection_num, :]\n",
    "\n",
    "                single_image_scores = single_image_scores.unsqueeze(0)\n",
    "                single_image_classes = single_image_classes.unsqueeze(0)\n",
    "                single_image_pred_bboxes = single_image_pred_bboxes.unsqueeze(0)\n",
    "\n",
    "                batch_scores.append(single_image_scores)\n",
    "                batch_classes.append(single_image_classes)\n",
    "                batch_pred_bboxes.append(single_image_pred_bboxes)\n",
    "\n",
    "            batch_scores = torch.cat(batch_scores, axis=0)\n",
    "            batch_classes = torch.cat(batch_classes, axis=0)\n",
    "            batch_pred_bboxes = torch.cat(batch_pred_bboxes, axis=0)\n",
    "\n",
    "            # batch_scores shape:[batch_size,max_detection_num]\n",
    "            # batch_classes shape:[batch_size,max_detection_num]\n",
    "            # batch_pred_bboxes shape[batch_size,max_detection_num,4]\n",
    "            box = torch.cat([batch_pred_bboxes[batch_pred_bboxes>=0].reshape(-1,4),batch_classes[batch_classes>=0].reshape(-1,1)],dim=1) \n",
    "            \n",
    "            return batch_scores, batch_classes, batch_pred_bboxes, box\n",
    "\n",
    "    def nms(self, one_image_scores, one_image_classes, one_image_pred_bboxes):\n",
    "        \"\"\"\n",
    "        one_image_scores:[anchor_nums],4:classification predict scores\n",
    "        one_image_classes:[anchor_nums],class indexes for predict scores\n",
    "        one_image_pred_bboxes:[anchor_nums,4],4:x_min,y_min,x_max,y_max\n",
    "        \"\"\"\n",
    "        # Sort boxes\n",
    "        sorted_one_image_scores, sorted_one_image_scores_indexes = torch.sort(one_image_scores, descending=True)\n",
    "        sorted_one_image_classes = one_image_classes[sorted_one_image_scores_indexes]\n",
    "        sorted_one_image_pred_bboxes = one_image_pred_bboxes[sorted_one_image_scores_indexes]\n",
    "        sorted_pred_bboxes_w_h = sorted_one_image_pred_bboxes[:,2:] - sorted_one_image_pred_bboxes[:, :2]\n",
    "\n",
    "        sorted_pred_bboxes_areas = sorted_pred_bboxes_w_h[:,0] * sorted_pred_bboxes_w_h[:,1]                                                                          \n",
    "        detected_classes = torch.unique(sorted_one_image_classes, sorted=True)\n",
    "\n",
    "        keep_scores, keep_classes, keep_pred_bboxes = [], [], []\n",
    "        for detected_class in detected_classes:\n",
    "            single_class_scores = sorted_one_image_scores[sorted_one_image_classes == detected_class]\n",
    "            single_class_pred_bboxes = sorted_one_image_pred_bboxes[sorted_one_image_classes == detected_class]\n",
    "            single_class_pred_bboxes_areas = sorted_pred_bboxes_areas[sorted_one_image_classes == detected_class]\n",
    "            single_class = sorted_one_image_classes[sorted_one_image_classes ==detected_class]\n",
    "\n",
    "            single_keep_scores,single_keep_classes,single_keep_pred_bboxes=[],[],[]\n",
    "            while single_class_scores.numel() > 0:\n",
    "                top1_score, top1_class, top1_pred_bbox = single_class_scores[\n",
    "                    0:1], single_class[0:1], single_class_pred_bboxes[0:1]\n",
    "\n",
    "                single_keep_scores.append(top1_score)\n",
    "                single_keep_classes.append(top1_class)\n",
    "                single_keep_pred_bboxes.append(top1_pred_bbox)\n",
    "\n",
    "                top1_areas = single_class_pred_bboxes_areas[0]\n",
    "\n",
    "                if single_class_scores.numel() == 1:\n",
    "                    break\n",
    "\n",
    "                single_class_scores = single_class_scores[1:]\n",
    "                single_class = single_class[1:]\n",
    "                single_class_pred_bboxes = single_class_pred_bboxes[1:]\n",
    "                single_class_pred_bboxes_areas = single_class_pred_bboxes_areas[1:]\n",
    "                    \n",
    "\n",
    "                overlap_area_top_left = torch.max( single_class_pred_bboxes[:, :2], top1_pred_bbox[:, :2])\n",
    "                   \n",
    "                overlap_area_bot_right = torch.min( single_class_pred_bboxes[:, 2:], top1_pred_bbox[:, 2:])\n",
    "                   \n",
    "                overlap_area_sizes = torch.clamp(overlap_area_bot_right -overlap_area_top_left,  min=0)\n",
    "                                                 \n",
    "                                               \n",
    "                overlap_area = overlap_area_sizes[:, 0] * overlap_area_sizes[:,1]\n",
    "                                                                             \n",
    "\n",
    "                # compute union_area\n",
    "                union_area = top1_areas + single_class_pred_bboxes_areas - overlap_area\n",
    "                union_area = torch.clamp(union_area, min=1e-4)\n",
    "                # compute ious for top1 pred_bbox and the other pred_bboxes\n",
    "                ious = overlap_area / union_area\n",
    "\n",
    "                single_class_scores = single_class_scores[ious < self.nms_threshold]\n",
    "                single_class = single_class[ious < self.nms_threshold]\n",
    "                single_class_pred_bboxes = single_class_pred_bboxes[ious < self.nms_threshold]\n",
    "                single_class_pred_bboxes_areas = single_class_pred_bboxes_areas[ious < self.nms_threshold]\n",
    "\n",
    "            single_keep_scores = torch.cat(single_keep_scores, axis=0)\n",
    "            single_keep_classes = torch.cat(single_keep_classes, axis=0)\n",
    "            single_keep_pred_bboxes = torch.cat(single_keep_pred_bboxes, axis=0)\n",
    "\n",
    "            keep_scores.append(single_keep_scores)\n",
    "            keep_classes.append(single_keep_classes)\n",
    "            keep_pred_bboxes.append(single_keep_pred_bboxes)\n",
    "\n",
    "        keep_scores = torch.cat(keep_scores, axis=0)\n",
    "        keep_classes = torch.cat(keep_classes, axis=0)\n",
    "        keep_pred_bboxes = torch.cat(keep_pred_bboxes, axis=0)\n",
    "\n",
    "        return keep_scores, keep_classes, keep_pred_bboxes\n",
    "\n",
    "    def snap_tx_ty_tw_th_reg_heads_to_x1_y1_x2_y2_bboxes(self, reg_heads, anchors):\n",
    "        \"\"\"\n",
    "        snap reg heads to pred bboxes\n",
    "        reg_heads:[anchor_nums,4],4:[tx,ty,tw,th]\n",
    "        anchors:[anchor_nums,4],4:[x_min,y_min,x_max,y_max]\n",
    "        \"\"\"\n",
    "        #Regress box\n",
    "        anchors_wh = anchors[:, 2:] - anchors[:, :2]\n",
    "        anchors_ctr = anchors[:, :2] + 0.5 * anchors_wh\n",
    "\n",
    "        device = anchors.device\n",
    "        factor = torch.tensor([[0.2, 0.2, 0.2, 0.2]]).to(device)\n",
    "\n",
    "        reg_heads = reg_heads * factor\n",
    "\n",
    "        pred_bboxes_wh = torch.exp(reg_heads[:, 2:]) * anchors_wh\n",
    "        pred_bboxes_ctr = reg_heads[:, :2] * anchors_wh + anchors_ctr\n",
    "\n",
    "        pred_bboxes_x_min_y_min = pred_bboxes_ctr - 0.5 * pred_bboxes_wh\n",
    "        pred_bboxes_x_max_y_max = pred_bboxes_ctr + 0.5 * pred_bboxes_wh\n",
    "\n",
    "        pred_bboxes = torch.cat([pred_bboxes_x_min_y_min, pred_bboxes_x_max_y_max], axis=1)\n",
    "        #pred_bboxes = pred_bboxes.int()\n",
    "        \n",
    "        #clamp box\n",
    "        pred_bboxes[:, 0] = torch.clamp(pred_bboxes[:, 0], min=0)\n",
    "        pred_bboxes[:, 1] = torch.clamp(pred_bboxes[:, 1], min=0)\n",
    "        pred_bboxes[:, 2] = torch.clamp(pred_bboxes[:, 2],max=self.image_w - 1)\n",
    "        pred_bboxes[:, 3] = torch.clamp(pred_bboxes[:, 3],max=self.image_h - 1)\n",
    "\n",
    "        # pred bboxes shape:[anchor_nums,4]\n",
    "        return pred_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "institutional-product",
   "metadata": {
    "code_folding": [
     0,
     14,
     29,
     70,
     104,
     116
    ]
   },
   "outputs": [],
   "source": [
    "# mAP calculation\n",
    "'''\n",
    "#N = number of samples\n",
    "#M = number of detections\n",
    "#P = number of annots\n",
    "#D = data (bbox and label)\n",
    "#num_categories = 8 #number of categories that can be detected\n",
    "#iou_range = np.linspace(0.5, 0.95, 10)\n",
    "\n",
    "#detections = [] #List of detections, with size [N * [M, D]]\n",
    "#annots = []     #List of annots, with size [N * [P, D]]\n",
    "\n",
    "'''\n",
    "\n",
    "def overlap_calc(detections_bbox, annots_bbox):\n",
    "    \"\"\"calculates the overlap between a detection Bbox and an annotation Bbox \"\"\"\n",
    "    \n",
    "    #for detections an annots size [D]\n",
    "    \n",
    "    x_overlap = max(0, (min(detections_bbox[2], annots_bbox[2]) - max(detections_bbox[0], annots_bbox[0])))\n",
    "    y_overlap = max(0, (min(detections_bbox[3], annots_bbox[3]) - max(detections_bbox[1], annots_bbox[1])))\n",
    "    overlap_area = x_overlap * y_overlap;\n",
    "    \n",
    "    annots_area = (annots_bbox[3] - annots_bbox[1]) * (annots_bbox[2] - annots_bbox[0])       \n",
    "    \n",
    "    overlap = overlap_area/annots_area\n",
    "    \n",
    "    return overlap\n",
    "\n",
    "def precision_recall(detections, annots, iou, category):\n",
    "    \"\"\"Calculates the precision and recall for a specific iou value \"\"\"\n",
    "\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    overlap = 0\n",
    "    num_annotation = 0\n",
    "    #for every sample with detections of the label\n",
    "    for n in range(len(detections)):\n",
    "        flag = 0;\n",
    "        #for every detection\n",
    "        for i in range(detections[n].shape[0]):\n",
    "            \n",
    "            if int(detections[n][i, 4].item()) == category:\n",
    "                max_overlap = 0\n",
    "                #Compare the detection with all the annots of the sample\n",
    "                for j in range(annots[n].shape[0]):\n",
    "                    if int(annots[n][j, 4].item()) == category:\n",
    "                        overlap = overlap_calc(detections[n][i,:], annots[n][j,:])\n",
    "                        max_overlap = max(max_overlap, overlap)\n",
    "                        if flag == 0:\n",
    "                            num_annotation+=1             \n",
    "                flag = 1\n",
    "                if max_overlap > iou:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    FP += 1\n",
    "    \n",
    "    if TP+FP==0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = TP/(TP+FP)\n",
    "    #calculate \n",
    "    if num_annotation==0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = TP/(num_annotation)\n",
    "    \n",
    "    return precision, recall\n",
    "    \n",
    "    \n",
    "def AP_calc(detections, annots, iou_range, category):\n",
    "    \"\"\"Calculates Average Precision (AP) for a specific label(category)\"\"\"\n",
    "    \n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    \n",
    "    for i in range(len(iou_range)):\n",
    "        precision, recall = precision_recall(detections, annots, iou_range[i], category)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "    \n",
    "    #print(precision_list,recall_list)\n",
    "\n",
    "    #recall_list = np.concatenate(([0.], recall_list, [1.]))\n",
    "    #precision_list = np.concatenate(([0.], precision_list, [0.]))\n",
    "\n",
    "    #AP = np.trapz(precision_list, recall_list)\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall_list, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision_list, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    AP = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return AP\n",
    "\n",
    "def total_instances_calc(annots, num_categories):\n",
    "    \n",
    "    total_instances = np.zeros(num_categories)\n",
    "    \n",
    "    #For every sample\n",
    "    for i in range(len(annots)):\n",
    "        #For every annotation in the sample\n",
    "        for j in range(annots[i].shape[0]):\n",
    "            total_instances[int(annots[i][j, 4].item())] += 1\n",
    "            \n",
    "    return  total_instances\n",
    "\n",
    "def mAP_calc(detections, annots, iou_range, num_categories):\n",
    "    \"\"\"Calculates the mean Average Precision (mAP)\"\"\"\n",
    "    \n",
    "    AP_list = np.empty(num_categories)\n",
    "    \n",
    "    #For each seperate category\n",
    "    for i in range(num_categories):\n",
    "        #Calculate the AP and append it to the AP_list\n",
    "        AP = AP_calc(detections, annots, iou_range,i) \n",
    "        AP_list[i] = AP\n",
    "    \n",
    "    #Amount of occurences of each category    \n",
    "    total_instances = total_instances_calc(annots, num_categories)\n",
    "        \n",
    "    #print(total_instances, AP_list)\n",
    "    mAP = np.sum(AP_list*total_instances) / sum(total_instances)\n",
    "    \n",
    "    return mAP\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-three",
   "metadata": {},
   "source": [
    "## Testing of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "twelve-treatment",
   "metadata": {
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-54c6c246a2ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mregression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_anchors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;31m#print(\"Regression output:\", crf_output[0].size(),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m#      \"Classification output:\",crf_output[1].size(),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7bfc409c0eca>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# FPN, input: C3, C4, C5, R3, R4, R5, R6, R7 , output: P3, P4, P5, P6, P7 (features)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackbone_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Detection Head, input: P4, P5, P6, P7, output:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2ac6aa23828b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs_dict, use_radar)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mR7\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rad_output_7'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mP5_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP5_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mP5_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP5_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP5_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CRF-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CRF-env\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CRF-env\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 396\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# crf test\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "crf_test = CRFNet().to(device)\n",
    "\n",
    "pretrained = True\n",
    "#print(crf_test)\n",
    "if pretrained == True:\n",
    "    # Load pretrained weight for model \n",
    "    import h5py\n",
    "\n",
    "    filename = \"crf_net.h5\"\n",
    "    model_order = [crf_test.fpn.P3_1, crf_test.fpn.P3_1,crf_test.fpn.P4_1, crf_test.fpn.P4_1,crf_test.fpn.P5_1, crf_test.fpn.P5_1,\n",
    "                   crf_test.fpn.P3_2, crf_test.fpn.P3_2,crf_test.fpn.P4_2, crf_test.fpn.P4_2,crf_test.fpn.P5_2, crf_test.fpn.P5_2,\n",
    "                   crf_test.fpn.P6, crf_test.fpn.P6,crf_test.fpn.P7_2, crf_test.fpn.P7_2, \n",
    "                   crf_test.backbone.blocks[0][0],crf_test.backbone.blocks[0][0],crf_test.backbone.blocks[0][2],crf_test.backbone.blocks[0][2],\n",
    "                   crf_test.backbone.blocks[1][0],crf_test.backbone.blocks[1][0],crf_test.backbone.blocks[1][2],crf_test.backbone.blocks[1][2],\n",
    "                   crf_test.backbone.blocks[2][0],crf_test.backbone.blocks[2][0],crf_test.backbone.blocks[2][2],crf_test.backbone.blocks[2][2],crf_test.backbone.blocks[2][4],crf_test.backbone.blocks[2][4],\n",
    "                   crf_test.backbone.blocks[3][0],crf_test.backbone.blocks[3][0],crf_test.backbone.blocks[3][2],crf_test.backbone.blocks[3][2],crf_test.backbone.blocks[3][4],crf_test.backbone.blocks[3][4],\n",
    "                   crf_test.backbone.blocks[4][0],crf_test.backbone.blocks[4][0],crf_test.backbone.blocks[4][2],crf_test.backbone.blocks[4][2],crf_test.backbone.blocks[4][4],crf_test.backbone.blocks[4][4],\n",
    "                   crf_test.classification.output,crf_test.classification.output,\n",
    "                   crf_test.classification.conv1,crf_test.classification.conv1,\n",
    "                   crf_test.classification.conv2,crf_test.classification.conv2,\n",
    "                   crf_test.classification.conv3,crf_test.classification.conv3,\n",
    "                   crf_test.classification.conv4,crf_test.classification.conv4,\n",
    "                   crf_test.regression.output,crf_test.regression.output,\n",
    "                   crf_test.regression.conv1,crf_test.regression.conv1,\n",
    "                   crf_test.regression.conv2,crf_test.regression.conv2,\n",
    "                   crf_test.regression.conv3,crf_test.regression.conv3,\n",
    "                   crf_test.regression.conv4,crf_test.regression.conv4]\n",
    "    h5 = h5py.File(filename,'r')\n",
    "    list(h5.keys())\n",
    "    model_weights = h5['model_weights'] \n",
    "    optimizer_weights = h5['optimizer_weights']  \n",
    "    def get_dataset_keys(f):\n",
    "        keys = []\n",
    "        f.visit(lambda key : keys.append(key) if type(f[key]) is h5py._hl.dataset.Dataset else None)\n",
    "        return keys\n",
    "    weight_keys = get_dataset_keys(model_weights)\n",
    "    for key, model in zip(weight_keys, model_order):\n",
    "        tensor_data = torch.tensor(np.transpose(model_weights[key][()]))\n",
    "        #print(key,tensor_data.shape)\n",
    "        para = nn.Parameter(tensor_data, requires_grad=True)\n",
    "        if 'bias' in key:\n",
    "            model.bias = para\n",
    "        if 'kernel' in key:\n",
    "            model.weight= para\n",
    "train_dataset = nuscenes_dataset(nusc,image_min_side=360,image_max_side=640)\n",
    "for i in range(1):\n",
    "    fused_image, annotations = train_dataset[i]\n",
    "    #print(\"Image shape: \",fused_image.shape, 'Annotation shape:',annotations.size())\n",
    "    test_img = fused_image\n",
    "    test_img.unsqueeze_(0)\n",
    "    annotations.unsqueeze_(0)\n",
    "    \n",
    "    if not annotations.shape[1]==0:\n",
    "        regression, classification, distance, batch_anchors = crf_test.forward(test_img)\n",
    "        #print(\"Regression output:\", crf_output[0].size(),\n",
    "        #      \"Classification output:\",crf_output[1].size(),\n",
    "        #      \"Distance output:\",crf_output[2].size(),\n",
    "        #     \"Anchors:\", crf_output[3].size())\n",
    "        #print(\"Annotation\", annotations)\n",
    "\n",
    "        # loss1 test\n",
    "        loss = RetinaLoss(image_w=640,image_h=360)\n",
    "        cls_loss, reg_loss = loss(classification, regression, batch_anchors, annotations)\n",
    "        print(cls_loss, reg_loss)\n",
    "\n",
    "        # Decoder test\n",
    "        decoder = RetinaDecoder(image_w=640,image_h=360) \n",
    "        batch_scores, batch_classes, batch_pred_bboxes, predicted_box = decoder(classification, regression, batch_anchors)\n",
    "        print(\"Image index:\", i,\"Number of detected box:\", len(batch_scores[batch_scores>0]), \"Number of annotation\", len(annotations[0,:,0]))\n",
    "        #print(\"Ground truth:\",'\\n', annotations)\n",
    "        #print(\"Prediction:\",'\\n', predicted_box)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-backing",
   "metadata": {
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# visualize\n",
    "def visualize_predictions(predictions, image_data_vis, dist=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Visualizes the predictions as bounding boxes with distances or confidence score in a given image.\n",
    "\n",
    "    :param predictions:         <list>              List with [bboxes, probs, labels]\n",
    "    :param image_data_vis:      <np.array>          Image where the predictions should be visualized\n",
    "    :param generator:           <Generator>         Data generator used for name to label mapping\n",
    "    :dist:                      <bool>              True if distance detection is enabled\n",
    "    :verbose:                   <bool>              True if detetions should be printed \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontScale              = 0.4\n",
    "\n",
    "    # Visualization prediction\n",
    "    all_dets = []\n",
    "    bboxes = predictions\n",
    "    print(bboxes.shape)\n",
    "    for jk in range(bboxes.shape[0]):\n",
    "        (x1, y1, x2, y2) = bboxes[jk,:]\n",
    "        \n",
    "   \n",
    "        cv2.rectangle(image_data_vis,(x1, y1), (x2, y2),2)\n",
    "\n",
    "image = cv2.imread('F:\\\\CameraRadarFusionNet\\\\crfnet\\\\data\\\\nuscenes\\\\samples\\\\CAM_FRONT\\\\n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402928112460.jpg')\n",
    "image = cv2.resize(image, (640,360))\n",
    "visualize_predictions(predicted_box[:,0:4],image)\n",
    "#visualize_predictions(annotations[0,:,0:4],image)\n",
    "cv2.imshow('image',image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-panic",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#backbone test\n",
    "train_dataset = nuscenes_dataset(nusc,image_min_side= 360,image_max_side = 640)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n",
    "fused_image, annotations = train_dataset[i]\n",
    "test_img = fused_image\n",
    "test_img.unsqueeze_(0)\n",
    "\n",
    "backbone_test = Backbone()\n",
    "backbone_output =  backbone_test.forward(test_img)\n",
    "#print(test_img)\n",
    "C3 = backbone_output['vgg_output_3'] \n",
    "C4 = backbone_output['vgg_output_4']\n",
    "C5 = backbone_output['vgg_output_5']\n",
    "R3 = backbone_output['rad_output_3']\n",
    "R4 = backbone_output['rad_output_4']\n",
    "R5 = backbone_output['rad_output_5']\n",
    "R6 = backbone_output['rad_output_6']\n",
    "R7 = backbone_output['rad_output_7']\n",
    "print(C3.shape,C4.shape,C5.shape)\n",
    "print(R3.shape,R4.shape,R5.shape,R6.shape,R7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-spider",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#FPN test\n",
    "fpn_test = PyramidFeatures()\n",
    "#print(fpn_test)\n",
    "fpn_output = fpn_test.forward(backbone_output)\n",
    "P3,P4,P5,P6,P7 = fpn_output\n",
    "print(P3.shape,P4.shape,P5.shape,P6.shape,P7.shape)\n",
    "\n",
    "#Detection head test\n",
    "regress_test= RegressionModel()\n",
    "class_test= ClassificationModel()\n",
    "regress_output=regress_test.forward(P3)\n",
    "print(regress_output.shape)\n",
    "class_output=class_test.forward(P3)\n",
    "print(class_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-productivity",
   "metadata": {},
   "source": [
    "## Training on the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "broadband-removal",
   "metadata": {
    "code_folding": [
     17,
     39
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "Epoch 1/20, start\n",
      "0 :\n",
      "cls_loss: 1.876 reg_loss: 0.963 total loss: 2.839\n",
      "1 :\n",
      "cls_loss: 1.211 reg_loss: 0.881 total loss: 2.092\n",
      "2 :\n",
      "cls_loss: 1.344 reg_loss: 0.829 total loss: 2.174\n",
      "3 :\n",
      "cls_loss: 1.144 reg_loss: 0.895 total loss: 2.040\n",
      "4 :\n",
      "cls_loss: 1.244 reg_loss: 0.881 total loss: 2.125\n",
      "5 :\n",
      "cls_loss: 1.011 reg_loss: 0.858 total loss: 1.869\n",
      "6 :\n",
      "cls_loss: 1.197 reg_loss: 0.931 total loss: 2.128\n",
      "7 :\n",
      "cls_loss: 1.366 reg_loss: 0.836 total loss: 2.202\n",
      "8 :\n",
      "cls_loss: 1.467 reg_loss: 0.791 total loss: 2.258\n",
      "9 :\n",
      "cls_loss: 1.498 reg_loss: 0.877 total loss: 2.375\n",
      "Epoch 1/20, Average Loss: 2.210, mAP: 0.001\n",
      "Epoch 2/20, start\n",
      "0 :\n",
      "cls_loss: 1.303 reg_loss: 0.792 total loss: 2.094\n",
      "1 :\n",
      "cls_loss: 0.779 reg_loss: 0.751 total loss: 1.530\n",
      "2 :\n",
      "cls_loss: 1.009 reg_loss: 0.687 total loss: 1.696\n",
      "3 :\n",
      "cls_loss: 0.780 reg_loss: 0.731 total loss: 1.510\n",
      "4 :\n",
      "cls_loss: 0.861 reg_loss: 0.750 total loss: 1.610\n",
      "5 :\n",
      "cls_loss: 0.774 reg_loss: 0.733 total loss: 1.507\n",
      "6 :\n",
      "cls_loss: 0.967 reg_loss: 0.819 total loss: 1.786\n",
      "7 :\n",
      "cls_loss: 1.133 reg_loss: 0.759 total loss: 1.892\n",
      "8 :\n",
      "cls_loss: 1.245 reg_loss: 0.700 total loss: 1.945\n",
      "9 :\n",
      "cls_loss: 1.232 reg_loss: 0.818 total loss: 2.050\n",
      "Epoch 2/20, Average Loss: 1.762, mAP: 0.012\n",
      "Epoch 3/20, start\n",
      "0 :\n",
      "cls_loss: 1.004 reg_loss: 0.726 total loss: 1.730\n",
      "1 :\n",
      "cls_loss: 0.654 reg_loss: 0.686 total loss: 1.340\n",
      "2 :\n",
      "cls_loss: 0.824 reg_loss: 0.644 total loss: 1.468\n",
      "3 :\n",
      "cls_loss: 0.637 reg_loss: 0.653 total loss: 1.290\n",
      "4 :\n",
      "cls_loss: 0.695 reg_loss: 0.692 total loss: 1.387\n",
      "5 :\n",
      "cls_loss: 0.588 reg_loss: 0.678 total loss: 1.266\n",
      "6 :\n",
      "cls_loss: 0.783 reg_loss: 0.756 total loss: 1.538\n",
      "7 :\n",
      "cls_loss: 0.946 reg_loss: 0.701 total loss: 1.647\n",
      "8 :\n",
      "cls_loss: 1.068 reg_loss: 0.641 total loss: 1.708\n",
      "9 :\n",
      "cls_loss: 1.066 reg_loss: 0.770 total loss: 1.836\n",
      "Epoch 3/20, Average Loss: 1.521, mAP: 0.090\n",
      "Epoch 4/20, start\n",
      "0 :\n",
      "cls_loss: 0.921 reg_loss: 0.674 total loss: 1.595\n",
      "1 :\n",
      "cls_loss: 0.547 reg_loss: 0.621 total loss: 1.168\n",
      "2 :\n",
      "cls_loss: 0.666 reg_loss: 0.569 total loss: 1.235\n",
      "3 :\n",
      "cls_loss: 0.431 reg_loss: 0.601 total loss: 1.032\n",
      "4 :\n",
      "cls_loss: 0.490 reg_loss: 0.633 total loss: 1.122\n",
      "5 :\n",
      "cls_loss: 0.413 reg_loss: 0.629 total loss: 1.041\n",
      "6 :\n",
      "cls_loss: 0.612 reg_loss: 0.707 total loss: 1.319\n",
      "7 :\n",
      "cls_loss: 0.772 reg_loss: 0.650 total loss: 1.422\n",
      "8 :\n",
      "cls_loss: 0.955 reg_loss: 0.588 total loss: 1.543\n",
      "9 :\n",
      "cls_loss: 0.965 reg_loss: 0.732 total loss: 1.697\n",
      "Epoch 4/20, Average Loss: 1.317, mAP: 0.339\n",
      "Epoch 5/20, start\n",
      "0 :\n",
      "cls_loss: 0.934 reg_loss: 0.616 total loss: 1.551\n",
      "1 :\n",
      "cls_loss: 0.747 reg_loss: 0.565 total loss: 1.313\n",
      "2 :\n",
      "cls_loss: 0.607 reg_loss: 0.520 total loss: 1.127\n",
      "3 :\n",
      "cls_loss: 0.311 reg_loss: 0.515 total loss: 0.826\n",
      "4 :\n",
      "cls_loss: 0.326 reg_loss: 0.581 total loss: 0.907\n",
      "5 :\n",
      "cls_loss: 0.312 reg_loss: 0.580 total loss: 0.893\n",
      "6 :\n",
      "cls_loss: 0.558 reg_loss: 0.660 total loss: 1.218\n",
      "7 :\n",
      "cls_loss: 0.722 reg_loss: 0.610 total loss: 1.332\n",
      "8 :\n",
      "cls_loss: 0.932 reg_loss: 0.542 total loss: 1.473\n",
      "9 :\n",
      "cls_loss: 0.952 reg_loss: 0.693 total loss: 1.645\n",
      "Epoch 5/20, Average Loss: 1.228, mAP: 0.434\n",
      "Epoch 6/20, start\n",
      "0 :\n",
      "cls_loss: 0.776 reg_loss: 0.568 total loss: 1.344\n",
      "1 :\n",
      "cls_loss: 0.522 reg_loss: 0.517 total loss: 1.039\n",
      "2 :\n",
      "cls_loss: 0.547 reg_loss: 0.472 total loss: 1.018\n",
      "3 :\n",
      "cls_loss: 0.336 reg_loss: 0.459 total loss: 0.795\n",
      "4 :\n",
      "cls_loss: 0.319 reg_loss: 0.531 total loss: 0.850\n",
      "5 :\n",
      "cls_loss: 0.268 reg_loss: 0.534 total loss: 0.802\n",
      "6 :\n",
      "cls_loss: 0.517 reg_loss: 0.616 total loss: 1.133\n",
      "7 :\n",
      "cls_loss: 0.646 reg_loss: 0.557 total loss: 1.203\n",
      "8 :\n",
      "cls_loss: 0.867 reg_loss: 0.504 total loss: 1.371\n",
      "9 :\n",
      "cls_loss: 0.888 reg_loss: 0.656 total loss: 1.544\n",
      "Epoch 6/20, Average Loss: 1.110, mAP: 0.497\n",
      "Epoch 7/20, start\n",
      "0 :\n",
      "cls_loss: 0.701 reg_loss: 0.496 total loss: 1.197\n",
      "1 :\n",
      "cls_loss: 0.480 reg_loss: 0.462 total loss: 0.942\n",
      "2 :\n",
      "cls_loss: 0.416 reg_loss: 0.398 total loss: 0.814\n",
      "3 :\n",
      "cls_loss: 0.269 reg_loss: 0.402 total loss: 0.672\n",
      "4 :\n",
      "cls_loss: 0.283 reg_loss: 0.484 total loss: 0.767\n",
      "5 :\n",
      "cls_loss: 0.259 reg_loss: 0.480 total loss: 0.739\n",
      "6 :\n",
      "cls_loss: 0.511 reg_loss: 0.576 total loss: 1.087\n",
      "7 :\n",
      "cls_loss: 0.589 reg_loss: 0.509 total loss: 1.099\n",
      "8 :\n",
      "cls_loss: 0.808 reg_loss: 0.471 total loss: 1.278\n",
      "9 :\n",
      "cls_loss: 0.829 reg_loss: 0.612 total loss: 1.441\n",
      "Epoch 7/20, Average Loss: 1.003, mAP: 0.583\n",
      "Epoch 8/20, start\n",
      "0 :\n",
      "cls_loss: 0.623 reg_loss: 0.456 total loss: 1.080\n",
      "1 :\n",
      "cls_loss: 0.437 reg_loss: 0.409 total loss: 0.846\n",
      "2 :\n",
      "cls_loss: 0.388 reg_loss: 0.344 total loss: 0.732\n",
      "3 :\n",
      "cls_loss: 0.209 reg_loss: 0.342 total loss: 0.552\n",
      "4 :\n",
      "cls_loss: 0.222 reg_loss: 0.435 total loss: 0.657\n",
      "5 :\n",
      "cls_loss: 0.174 reg_loss: 0.428 total loss: 0.602\n",
      "6 :\n",
      "cls_loss: 0.474 reg_loss: 0.537 total loss: 1.011\n",
      "7 :\n",
      "cls_loss: 0.535 reg_loss: 0.458 total loss: 0.993\n",
      "8 :\n",
      "cls_loss: 0.768 reg_loss: 0.440 total loss: 1.209\n",
      "9 :\n",
      "cls_loss: 0.791 reg_loss: 0.569 total loss: 1.360\n",
      "Epoch 8/20, Average Loss: 0.904, mAP: 0.681\n",
      "Epoch 9/20, start\n",
      "0 :\n",
      "cls_loss: 0.551 reg_loss: 0.394 total loss: 0.945\n",
      "1 :\n",
      "cls_loss: 0.406 reg_loss: 0.358 total loss: 0.765\n",
      "2 :\n",
      "cls_loss: 0.351 reg_loss: 0.290 total loss: 0.641\n",
      "3 :\n",
      "cls_loss: 0.187 reg_loss: 0.299 total loss: 0.486\n",
      "4 :\n",
      "cls_loss: 0.198 reg_loss: 0.398 total loss: 0.596\n",
      "5 :\n",
      "cls_loss: 0.154 reg_loss: 0.376 total loss: 0.530\n",
      "6 :\n",
      "cls_loss: 0.441 reg_loss: 0.492 total loss: 0.934\n",
      "7 :\n",
      "cls_loss: 0.499 reg_loss: 0.424 total loss: 0.922\n",
      "8 :\n",
      "cls_loss: 0.717 reg_loss: 0.406 total loss: 1.123\n",
      "9 :\n",
      "cls_loss: 0.750 reg_loss: 0.523 total loss: 1.273\n",
      "Epoch 9/20, Average Loss: 0.821, mAP: 0.700\n",
      "Epoch 10/20, start\n",
      "0 :\n",
      "cls_loss: 0.500 reg_loss: 0.349 total loss: 0.849\n",
      "1 :\n",
      "cls_loss: 0.387 reg_loss: 0.337 total loss: 0.723\n",
      "2 :\n",
      "cls_loss: 0.339 reg_loss: 0.254 total loss: 0.593\n",
      "3 :\n",
      "cls_loss: 0.166 reg_loss: 0.276 total loss: 0.442\n",
      "4 :\n",
      "cls_loss: 0.175 reg_loss: 0.365 total loss: 0.540\n",
      "5 :\n",
      "cls_loss: 0.139 reg_loss: 0.347 total loss: 0.486\n",
      "6 :\n",
      "cls_loss: 0.416 reg_loss: 0.465 total loss: 0.881\n",
      "7 :\n",
      "cls_loss: 0.466 reg_loss: 0.384 total loss: 0.851\n",
      "8 :\n",
      "cls_loss: 0.673 reg_loss: 0.358 total loss: 1.031\n",
      "9 :\n",
      "cls_loss: 0.717 reg_loss: 0.485 total loss: 1.202\n",
      "Epoch 10/20, Average Loss: 0.760, mAP: 0.669\n",
      "Epoch 11/20, start\n",
      "0 :\n",
      "cls_loss: 0.473 reg_loss: 0.305 total loss: 0.778\n",
      "1 :\n",
      "cls_loss: 0.367 reg_loss: 0.358 total loss: 0.725\n",
      "2 :\n",
      "cls_loss: 0.306 reg_loss: 0.240 total loss: 0.546\n",
      "3 :\n",
      "cls_loss: 0.144 reg_loss: 0.230 total loss: 0.374\n",
      "4 :\n",
      "cls_loss: 0.158 reg_loss: 0.335 total loss: 0.494\n",
      "5 :\n",
      "cls_loss: 0.124 reg_loss: 0.334 total loss: 0.458\n",
      "6 :\n",
      "cls_loss: 0.390 reg_loss: 0.442 total loss: 0.831\n",
      "7 :\n",
      "cls_loss: 0.434 reg_loss: 0.377 total loss: 0.811\n",
      "8 :\n",
      "cls_loss: 0.626 reg_loss: 0.350 total loss: 0.975\n",
      "9 :\n",
      "cls_loss: 0.677 reg_loss: 0.447 total loss: 1.124\n",
      "Epoch 11/20, Average Loss: 0.712, mAP: 0.726\n",
      "Epoch 12/20, start\n",
      "0 :\n",
      "cls_loss: 0.439 reg_loss: 0.260 total loss: 0.699\n",
      "1 :\n",
      "cls_loss: 0.355 reg_loss: 0.296 total loss: 0.651\n",
      "2 :\n",
      "cls_loss: 0.285 reg_loss: 0.184 total loss: 0.469\n",
      "3 :\n",
      "cls_loss: 0.131 reg_loss: 0.194 total loss: 0.325\n",
      "4 :\n",
      "cls_loss: 0.144 reg_loss: 0.293 total loss: 0.437\n",
      "5 :\n",
      "cls_loss: 0.107 reg_loss: 0.323 total loss: 0.431\n",
      "6 :\n",
      "cls_loss: 0.359 reg_loss: 0.409 total loss: 0.768\n",
      "7 :\n",
      "cls_loss: 0.392 reg_loss: 0.337 total loss: 0.729\n",
      "8 :\n",
      "cls_loss: 0.570 reg_loss: 0.308 total loss: 0.878\n",
      "9 :\n",
      "cls_loss: 0.630 reg_loss: 0.411 total loss: 1.041\n",
      "Epoch 12/20, Average Loss: 0.643, mAP: 0.662\n",
      "Epoch 13/20, start\n",
      "0 :\n",
      "cls_loss: 0.416 reg_loss: 0.227 total loss: 0.642\n",
      "1 :\n",
      "cls_loss: 0.336 reg_loss: 0.217 total loss: 0.554\n",
      "2 :\n",
      "cls_loss: 0.255 reg_loss: 0.163 total loss: 0.418\n",
      "3 :\n",
      "cls_loss: 0.110 reg_loss: 0.205 total loss: 0.314\n",
      "4 :\n",
      "cls_loss: 0.119 reg_loss: 0.247 total loss: 0.366\n",
      "5 :\n",
      "cls_loss: 0.090 reg_loss: 0.251 total loss: 0.341\n",
      "6 :\n",
      "cls_loss: 0.307 reg_loss: 0.369 total loss: 0.676\n",
      "7 :\n",
      "cls_loss: 0.321 reg_loss: 0.298 total loss: 0.619\n",
      "8 :\n",
      "cls_loss: 0.483 reg_loss: 0.274 total loss: 0.757\n",
      "9 :\n",
      "cls_loss: 0.550 reg_loss: 0.377 total loss: 0.927\n",
      "Epoch 13/20, Average Loss: 0.561, mAP: 0.651\n",
      "Epoch 14/20, start\n",
      "0 :\n",
      "cls_loss: 0.391 reg_loss: 0.214 total loss: 0.605\n",
      "1 :\n",
      "cls_loss: 0.322 reg_loss: 0.200 total loss: 0.521\n",
      "2 :\n",
      "cls_loss: 0.209 reg_loss: 0.196 total loss: 0.406\n",
      "3 :\n",
      "cls_loss: 0.081 reg_loss: 0.195 total loss: 0.276\n",
      "4 :\n",
      "cls_loss: 0.078 reg_loss: 0.210 total loss: 0.287\n",
      "5 :\n",
      "cls_loss: 0.065 reg_loss: 0.207 total loss: 0.272\n",
      "6 :\n",
      "cls_loss: 0.197 reg_loss: 0.346 total loss: 0.543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 :\n",
      "cls_loss: 0.193 reg_loss: 0.287 total loss: 0.480\n",
      "8 :\n",
      "cls_loss: 0.347 reg_loss: 0.268 total loss: 0.615\n",
      "9 :\n",
      "cls_loss: 0.412 reg_loss: 0.349 total loss: 0.761\n",
      "Epoch 14/20, Average Loss: 0.477, mAP: 0.593\n",
      "Epoch 15/20, start\n",
      "0 :\n",
      "cls_loss: 0.399 reg_loss: 0.196 total loss: 0.595\n",
      "1 :\n",
      "cls_loss: 0.427 reg_loss: 0.304 total loss: 0.731\n",
      "2 :\n",
      "cls_loss: 0.202 reg_loss: 0.186 total loss: 0.388\n",
      "3 :\n",
      "cls_loss: 0.087 reg_loss: 0.140 total loss: 0.227\n",
      "4 :\n",
      "cls_loss: 0.064 reg_loss: 0.177 total loss: 0.241\n",
      "5 :\n",
      "cls_loss: 0.059 reg_loss: 0.196 total loss: 0.255\n",
      "6 :\n",
      "cls_loss: 0.211 reg_loss: 0.339 total loss: 0.550\n",
      "7 :\n",
      "cls_loss: 0.205 reg_loss: 0.305 total loss: 0.510\n",
      "8 :\n",
      "cls_loss: 0.352 reg_loss: 0.296 total loss: 0.647\n",
      "9 :\n",
      "cls_loss: 0.450 reg_loss: 0.342 total loss: 0.793\n",
      "Epoch 15/20, Average Loss: 0.494, mAP: 0.551\n",
      "Epoch 16/20, start\n",
      "0 :\n",
      "cls_loss: 0.345 reg_loss: 0.187 total loss: 0.532\n",
      "1 :\n",
      "cls_loss: 0.276 reg_loss: 0.183 total loss: 0.459\n",
      "2 :\n",
      "cls_loss: 0.190 reg_loss: 0.132 total loss: 0.322\n",
      "3 :\n",
      "cls_loss: 0.064 reg_loss: 0.138 total loss: 0.202\n",
      "4 :\n",
      "cls_loss: 0.075 reg_loss: 0.184 total loss: 0.260\n",
      "5 :\n",
      "cls_loss: 0.061 reg_loss: 0.187 total loss: 0.248\n",
      "6 :\n",
      "cls_loss: 0.182 reg_loss: 0.294 total loss: 0.476\n",
      "7 :\n",
      "cls_loss: 0.163 reg_loss: 0.261 total loss: 0.425\n",
      "8 :\n",
      "cls_loss: 0.281 reg_loss: 0.248 total loss: 0.528\n",
      "9 :\n",
      "cls_loss: 0.364 reg_loss: 0.312 total loss: 0.676\n",
      "Epoch 16/20, Average Loss: 0.413, mAP: 0.551\n",
      "Epoch 17/20, start\n",
      "0 :\n",
      "cls_loss: 0.308 reg_loss: 0.183 total loss: 0.491\n",
      "1 :\n",
      "cls_loss: 0.226 reg_loss: 0.196 total loss: 0.422\n",
      "2 :\n",
      "cls_loss: 0.164 reg_loss: 0.139 total loss: 0.303\n",
      "3 :\n",
      "cls_loss: 0.063 reg_loss: 0.098 total loss: 0.160\n",
      "4 :\n",
      "cls_loss: 0.056 reg_loss: 0.152 total loss: 0.208\n",
      "5 :\n",
      "cls_loss: 0.058 reg_loss: 0.167 total loss: 0.225\n",
      "6 :\n",
      "cls_loss: 0.105 reg_loss: 0.257 total loss: 0.362\n",
      "7 :\n",
      "cls_loss: 0.125 reg_loss: 0.230 total loss: 0.355\n",
      "8 :\n",
      "cls_loss: 0.211 reg_loss: 0.258 total loss: 0.470\n",
      "9 :\n",
      "cls_loss: 0.279 reg_loss: 0.283 total loss: 0.562\n",
      "Epoch 17/20, Average Loss: 0.356, mAP: 0.503\n",
      "Epoch 18/20, start\n",
      "0 :\n",
      "cls_loss: 0.275 reg_loss: 0.144 total loss: 0.418\n",
      "1 :\n",
      "cls_loss: 0.176 reg_loss: 0.187 total loss: 0.363\n",
      "2 :\n",
      "cls_loss: 0.128 reg_loss: 0.126 total loss: 0.254\n",
      "3 :\n",
      "cls_loss: 0.047 reg_loss: 0.114 total loss: 0.161\n",
      "4 :\n",
      "cls_loss: 0.036 reg_loss: 0.137 total loss: 0.173\n",
      "5 :\n",
      "cls_loss: 0.047 reg_loss: 0.149 total loss: 0.196\n",
      "6 :\n",
      "cls_loss: 0.089 reg_loss: 0.253 total loss: 0.342\n",
      "7 :\n",
      "cls_loss: 0.081 reg_loss: 0.223 total loss: 0.304\n",
      "8 :\n",
      "cls_loss: 0.189 reg_loss: 0.201 total loss: 0.390\n",
      "9 :\n",
      "cls_loss: 0.237 reg_loss: 0.260 total loss: 0.497\n",
      "Epoch 18/20, Average Loss: 0.310, mAP: 0.531\n",
      "Epoch 19/20, start\n",
      "0 :\n",
      "cls_loss: 0.232 reg_loss: 0.115 total loss: 0.347\n",
      "1 :\n",
      "cls_loss: 0.131 reg_loss: 0.153 total loss: 0.283\n",
      "2 :\n",
      "cls_loss: 0.087 reg_loss: 0.094 total loss: 0.181\n",
      "3 :\n",
      "cls_loss: 0.050 reg_loss: 0.107 total loss: 0.157\n",
      "4 :\n",
      "cls_loss: 0.038 reg_loss: 0.127 total loss: 0.164\n",
      "5 :\n",
      "cls_loss: 0.055 reg_loss: 0.149 total loss: 0.203\n",
      "6 :\n",
      "cls_loss: 0.066 reg_loss: 0.210 total loss: 0.275\n",
      "7 :\n",
      "cls_loss: 0.063 reg_loss: 0.184 total loss: 0.247\n",
      "8 :\n",
      "cls_loss: 0.155 reg_loss: 0.183 total loss: 0.338\n",
      "9 :\n",
      "cls_loss: 0.214 reg_loss: 0.235 total loss: 0.449\n",
      "Epoch 19/20, Average Loss: 0.265, mAP: 0.459\n",
      "Epoch 20/20, start\n",
      "0 :\n",
      "cls_loss: 0.178 reg_loss: 0.125 total loss: 0.303\n",
      "1 :\n",
      "cls_loss: 0.109 reg_loss: 0.135 total loss: 0.244\n",
      "2 :\n",
      "cls_loss: 0.069 reg_loss: 0.098 total loss: 0.166\n",
      "3 :\n",
      "cls_loss: 0.033 reg_loss: 0.072 total loss: 0.105\n",
      "4 :\n",
      "cls_loss: 0.031 reg_loss: 0.114 total loss: 0.145\n",
      "5 :\n",
      "cls_loss: 0.028 reg_loss: 0.125 total loss: 0.153\n",
      "6 :\n",
      "cls_loss: 0.070 reg_loss: 0.203 total loss: 0.273\n",
      "7 :\n",
      "cls_loss: 0.053 reg_loss: 0.171 total loss: 0.224\n",
      "8 :\n",
      "cls_loss: 0.124 reg_loss: 0.182 total loss: 0.307\n",
      "9 :\n",
      "cls_loss: 0.184 reg_loss: 0.219 total loss: 0.403\n",
      "Epoch 20/20, Average Loss: 0.232, mAP: 0.509\n"
     ]
    }
   ],
   "source": [
    "# training stage\n",
    "max_epochs = 20\n",
    "batch_size = 1\n",
    "num_workers = 4\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "# crf test\n",
    "crf_test = CRFNet().to(device)\n",
    "# pretrain weights\n",
    "pretrained = True\n",
    "if pretrained == True:\n",
    "    # Load pretrained weight for model \n",
    "    import h5py\n",
    "\n",
    "    filename = \"crf_net.h5\"\n",
    "    model_order = [crf_test.fpn.P3_1, crf_test.fpn.P3_1,crf_test.fpn.P4_1, crf_test.fpn.P4_1,crf_test.fpn.P5_1, crf_test.fpn.P5_1,\n",
    "                   crf_test.fpn.P3_2, crf_test.fpn.P3_2,crf_test.fpn.P4_2, crf_test.fpn.P4_2,crf_test.fpn.P5_2, crf_test.fpn.P5_2,\n",
    "                   crf_test.fpn.P6, crf_test.fpn.P6,crf_test.fpn.P7_2, crf_test.fpn.P7_2, \n",
    "                   crf_test.backbone.blocks[0][0],crf_test.backbone.blocks[0][0],crf_test.backbone.blocks[0][2],crf_test.backbone.blocks[0][2],\n",
    "                   crf_test.backbone.blocks[1][0],crf_test.backbone.blocks[1][0],crf_test.backbone.blocks[1][2],crf_test.backbone.blocks[1][2],\n",
    "                   crf_test.backbone.blocks[2][0],crf_test.backbone.blocks[2][0],crf_test.backbone.blocks[2][2],crf_test.backbone.blocks[2][2],crf_test.backbone.blocks[2][4],crf_test.backbone.blocks[2][4],\n",
    "                   crf_test.backbone.blocks[3][0],crf_test.backbone.blocks[3][0],crf_test.backbone.blocks[3][2],crf_test.backbone.blocks[3][2],crf_test.backbone.blocks[3][4],crf_test.backbone.blocks[3][4],\n",
    "                   crf_test.backbone.blocks[4][0],crf_test.backbone.blocks[4][0],crf_test.backbone.blocks[4][2],crf_test.backbone.blocks[4][2],crf_test.backbone.blocks[4][4],crf_test.backbone.blocks[4][4],\n",
    "                   crf_test.classification.output,crf_test.classification.output,\n",
    "                   crf_test.classification.conv1,crf_test.classification.conv1,\n",
    "                   crf_test.classification.conv2,crf_test.classification.conv2,\n",
    "                   crf_test.classification.conv3,crf_test.classification.conv3,\n",
    "                   crf_test.classification.conv4,crf_test.classification.conv4,\n",
    "                   crf_test.regression.output,crf_test.regression.output,\n",
    "                   crf_test.regression.conv1,crf_test.regression.conv1,\n",
    "                   crf_test.regression.conv2,crf_test.regression.conv2,\n",
    "                   crf_test.regression.conv3,crf_test.regression.conv3,\n",
    "                   crf_test.regression.conv4,crf_test.regression.conv4]\n",
    "    h5 = h5py.File(filename,'r')\n",
    "    list(h5.keys())\n",
    "    model_weights = h5['model_weights'] \n",
    "    optimizer_weights = h5['optimizer_weights']  \n",
    "    def get_dataset_keys(f):\n",
    "        keys = []\n",
    "        f.visit(lambda key : keys.append(key) if type(f[key]) is h5py._hl.dataset.Dataset else None)\n",
    "        return keys\n",
    "    weight_keys = get_dataset_keys(model_weights)\n",
    "    for key, model in zip(weight_keys, model_order):\n",
    "        #if 'block' in key:\n",
    "        tensor_data = torch.tensor(np.transpose(model_weights[key][()])).to(device)\n",
    "        #print(key,tensor_data.shape)\n",
    "        para = nn.Parameter(tensor_data, requires_grad=True)\n",
    "        if 'bias' in key:\n",
    "            model.bias = para\n",
    "        if 'kernel' in key:\n",
    "            model.weight= para\n",
    "\n",
    "# loss test\n",
    "criterion = RetinaLoss(image_w=360,image_h=640).to(device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Optimizer:\n",
    "optimizer = torch.optim.Adam(crf_test.parameters(), lr=2*(10**-5))\n",
    "\n",
    "training_set = nuscenes_dataset(nusc,image_min_side= 360,image_max_side = 640)\n",
    "training_generator = DataLoader(dataset=training_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Decoder\n",
    "decoder = RetinaDecoder(image_w=640,image_h=360)\n",
    "num_categories = 8 #number of categories that can be detected\n",
    "iou_range = np.linspace(0.5, 0.95, 10)\n",
    "\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    index = 0\n",
    "    loss_sum = 0\n",
    "    detections = [] #List of detections, with size [N * [M, D]]\n",
    "    annotations = []     #List of annots, with size [N * [P, D]]\n",
    "    print(\"Epoch {}/{}, start\".format(epoch+1, max_epochs))\n",
    "    for img_plus, annots in training_generator:\n",
    "\n",
    "            print(index, \":\")\n",
    "            if not annots.shape[1]==0:\n",
    "                \n",
    "                img_plus = img_plus.to(device)\n",
    "                annots = annots.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                #print(img_plus.shape)\n",
    "\n",
    "                output = crf_test(img_plus)\n",
    "                #print(\"output calculated\")\n",
    "\n",
    "                #print(annots)\n",
    "                cls_loss, reg_loss = criterion(output[1], output[0], output[3], annots) \n",
    "                loss = cls_loss + reg_loss\n",
    "                print('cls_loss:',\"%.3f\" % cls_loss.item(),'reg_loss:', \"%.3f\" % reg_loss.item(),'total loss:', \"%.3f\" % loss.item())\n",
    "\n",
    "                loss_sum += loss.item()\n",
    "                if not cls_loss.grad_fn == None:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    index += 1\n",
    "                    _, _, _, predicted_box = decoder(output[1], output[0], output[3])\n",
    "                    detections.append(predicted_box) #List of detections, with size [N * [M, D]]\n",
    "                    annotations.append(annots.squeeze())  #List of annots, with size [N * [P, D]]\n",
    "                if index==10:\n",
    "                    break\n",
    "    \n",
    "    mAP = mAP_calc(detections, annotations, iou_range, num_categories)\n",
    "    #print epoch and corresponding loss\n",
    "    print(\"Epoch {}/{}, Average Loss: {:.3f}, mAP: {:.3f}\".format(epoch+1, max_epochs, loss_sum/index, mAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-destruction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
